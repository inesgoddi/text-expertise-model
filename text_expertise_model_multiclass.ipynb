{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install nltk\n",
        "!pip install nltk matplotlib\n",
        "train_df = pd.read_csv(\"train.csv\", on_bad_lines='skip')  # Skip bad lines\n",
        "import pandas as pd\n",
        "\n",
        "# Read the Excel file\n",
        "third_df = pd.read_excel(\"Book.xlsx\")  # or .xls if it's an older format\n",
        "  # Skip bad lines\n",
        "val_df = pd.read_csv(\"val.csv\", on_bad_lines='skip')  # Skip bad lines\n",
        "test_df = pd.read_csv(\"test.csv\", on_bad_lines='skip')\n",
        "merged_df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
        "\n",
        "merged_df_2 = pd.concat([train_df, third_df], ignore_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ynjf1UjR4fG1",
        "outputId": "d008a111-9969-4d35-d3ad-6fd0ca0244fc"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# First, split off the train set with stratification\n",
        "train_df, temp_df = train_test_split(\n",
        "    merged_df_2, test_size=0.3, stratify=merged_df_2['label'], random_state=42\n",
        ")\n",
        "\n",
        "# Then split the temp into val and test (still stratified)\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "# Reset and reindex train_df\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "train_df.index += 1\n",
        "\n",
        "# Reset and reindex val_df\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "val_df.index += 1\n",
        "\n",
        "# Reset and reindex test_df\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "test_df.index += 1\n",
        "# Check the distribution\n",
        "print(\"Train distribution:\\n\", train_df['label'].value_counts(normalize=True))\n",
        "print(\"Val distribution:\\n\", val_df['label'].value_counts(normalize=True))\n",
        "print(\"Test distribution:\\n\", test_df['label'].value_counts(normalize=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bEV-KJbCHYv",
        "outputId": "5b3e1185-cc95-4cad-e508-dd5d9baa3d15"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train distribution:\n",
            " label\n",
            "1    0.337617\n",
            "0    0.335280\n",
            "2    0.327103\n",
            "Name: proportion, dtype: float64\n",
            "Val distribution:\n",
            " label\n",
            "1    0.342391\n",
            "0    0.331522\n",
            "2    0.326087\n",
            "Name: proportion, dtype: float64\n",
            "Test distribution:\n",
            " label\n",
            "1    0.336957\n",
            "0    0.336957\n",
            "2    0.326087\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frist method : BERT pre-trained"
      ],
      "metadata": {
        "id": "YEDSyVtpO630"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install CUDA Toolkit\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-11-0_11.0.3-1_amd64.deb\n",
        "!dpkg -i cuda-11-0_11.0.3-1_amd64.deb\n",
        "!apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\n",
        "!apt-get update\n",
        "!apt-get install cuda\n",
        "\n",
        "# Install cuDNN (adjust version accordingly)\n",
        "!wget https://developer.download.nvidia.com/compute/redist/cudnn/v8.0.5/cudnn-11.0-linux-x64-v8.0.5.39.tgz\n",
        "!tar -xzvf cudnn-11.0-linux-x64-v8.0.5.39.tgz\n",
        "!cp cuda/include/cudnn*.h /usr/local/cuda/include\n",
        "!cp cuda/lib64/libcudnn* /usr/local/cuda/lib64\n",
        "!chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn*\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "cdpB3sDEPCvJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6abc9848-f49b-4feb-fd89-d196d7a30aec"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-04 23:04:56--  https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-11-0_11.0.3-1_amd64.deb\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 23.213.43.209, 23.213.43.221\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|23.213.43.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2446 (2.4K) [application/x-deb]\n",
            "Saving to: ‘cuda-11-0_11.0.3-1_amd64.deb’\n",
            "\n",
            "cuda-11-0_11.0.3-1_ 100%[===================>]   2.39K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-04-04 23:04:57 (175 MB/s) - ‘cuda-11-0_11.0.3-1_amd64.deb’ saved [2446/2446]\n",
            "\n",
            "Selecting previously unselected package cuda-11-0.\n",
            "(Reading database ... 126213 files and directories currently installed.)\n",
            "Preparing to unpack cuda-11-0_11.0.3-1_amd64.deb ...\n",
            "Unpacking cuda-11-0 (11.0.3-1) ...\n",
            "\u001b[1mdpkg:\u001b[0m dependency problems prevent configuration of cuda-11-0:\n",
            " cuda-11-0 depends on cuda-runtime-11-0 (>= 11.0.3); however:\n",
            "  Package cuda-runtime-11-0 is not installed.\n",
            " cuda-11-0 depends on cuda-toolkit-11-0 (>= 11.0.3); however:\n",
            "  Package cuda-toolkit-11-0 is not installed.\n",
            " cuda-11-0 depends on cuda-demo-suite-11-0 (>= 11.0.167); however:\n",
            "  Package cuda-demo-suite-11-0 is not installed.\n",
            "\n",
            "\u001b[1mdpkg:\u001b[0m error processing package cuda-11-0 (--install):\n",
            " dependency problems - leaving unconfigured\n",
            "Errors were encountered while processing:\n",
            " cuda-11-0\n",
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "Executing: /tmp/apt-key-gpghome.bDV26B1yb6/gpg.1.sh --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\n",
            "gpg: requesting key from 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub'\n",
            "gpg: key F60F4B3D7FA2AF80: public key \"cudatools <cudatools@nvidia.com>\" imported\n",
            "gpg: Total number processed: 1\n",
            "gpg:               imported: 1\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,381 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,686 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,241 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,540 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,978 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,148 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,092 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,775 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,812 kB]\n",
            "Fetched 30.0 MB in 4s (7,337 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "You might want to run 'apt --fix-broken install' to correct these.\n",
            "The following packages have unmet dependencies:\n",
            " cuda : Depends: cuda-12-8 (>= 12.8.1) but it is not going to be installed\n",
            "        Depends: nvidia-open (>= 570.124.06) but it is not going to be installed\n",
            " cuda-11-0 : Depends: cuda-runtime-11-0 (>= 11.0.3) but it is not installable\n",
            "             Depends: cuda-toolkit-11-0 (>= 11.0.3) but it is not installable\n",
            "             Depends: cuda-demo-suite-11-0 (>= 11.0.167) but it is not installable\n",
            "E: Unmet dependencies. Try 'apt --fix-broken install' with no packages (or specify a solution).\n",
            "--2025-04-04 23:05:09--  https://developer.download.nvidia.com/compute/redist/cudnn/v8.0.5/cudnn-11.0-linux-x64-v8.0.5.39.tgz\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 23.213.43.209, 23.213.43.221\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|23.213.43.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1085996495 (1.0G) [application/x-compressed]\n",
            "Saving to: ‘cudnn-11.0-linux-x64-v8.0.5.39.tgz’\n",
            "\n",
            ".39.tgz              25%[====>               ] 262.00M  2.51MB/s    eta 5m 18s ^C\n",
            "cuda/include/cudnn.h\n",
            "cuda/include/cudnn_adv_infer.h\n",
            "cuda/include/cudnn_adv_train.h\n",
            "cuda/include/cudnn_backend.h\n",
            "cuda/include/cudnn_cnn_infer.h\n",
            "cuda/include/cudnn_cnn_train.h\n",
            "cuda/include/cudnn_ops_infer.h\n",
            "cuda/include/cudnn_ops_train.h\n",
            "cuda/include/cudnn_version.h\n",
            "cuda/NVIDIA_SLA_cuDNN_Support.txt\n",
            "cuda/lib64/libcudnn.so\n",
            "cuda/lib64/libcudnn.so.8\n",
            "cuda/lib64/libcudnn.so.8.0.5\n",
            "cuda/lib64/libcudnn_adv_infer.so\n",
            "cuda/lib64/libcudnn_adv_infer.so.8\n",
            "cuda/lib64/libcudnn_adv_infer.so.8.0.5\n",
            "cuda/lib64/libcudnn_adv_train.so\n",
            "cuda/lib64/libcudnn_adv_train.so.8\n",
            "cuda/lib64/libcudnn_adv_train.so.8.0.5\n",
            "cuda/lib64/libcudnn_cnn_infer.so\n",
            "cuda/lib64/libcudnn_cnn_infer.so.8\n",
            "cuda/lib64/libcudnn_cnn_infer.so.8.0.5\n",
            "\n",
            "gzip: stdin: unexpected end of file\n",
            "tar: Unexpected EOF in archive\n",
            "tar: Unexpected EOF in archive\n",
            "tar: Error is not recoverable: exiting now\n",
            "Fri Apr  4 23:07:11 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0             43W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Load the pretrained BERT model and tokenizer (without classification head)\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def chunk_text_func(input_text, chunk_size=510):\n",
        "    \"\"\"\n",
        "    Split the input text into non-overlapping chunks of specified size.\n",
        "    The chunking is done by tokenizing the text first, then splitting into chunks.\n",
        "    \"\"\"\n",
        "    tokens = tokenizer.tokenize(input_text)\n",
        "\n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), chunk_size):\n",
        "        chunk = tokens[i:i + chunk_size]\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def extract_cls_embeddings(text,max_length=512):\n",
        "    \"\"\"\n",
        "    Extracts [CLS] embeddings for each chunk of text.\n",
        "\n",
        "    :param text: The text input to process.\n",
        "    :param chunk_size: The chunk size for splitting long text.\n",
        "    :return: List of CLS embeddings for each chunk of text.\n",
        "    \"\"\"\n",
        "    chunks = chunk_text_func(text)\n",
        "    cls_embeddings = []\n",
        "\n",
        "\n",
        "    for chunk in chunks:\n",
        "\n",
        "        inputs = tokenizer.encode_plus(\n",
        "            chunk,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors='pt',\n",
        "            padding='max_length',\n",
        "            max_length=max_length\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
        "        cls_embeddings.append(cls_embedding)\n",
        "\n",
        "    return cls_embeddings\n",
        "\n",
        "\n",
        "def process_texts(df, label):\n",
        "    \"\"\"\n",
        "    Process texts in a dataframe and return aggregated [CLS] embeddings.\n",
        "\n",
        "    :param df: DataFrame containing the text and label columns.\n",
        "    :param label: The label (0 for beginner, 1 for expert) to filter the dataframe.\n",
        "    :param chunk_size: The chunk size for splitting long text.\n",
        "    :return: Aggregated mean [CLS] embeddings for the given label.\n",
        "    \"\"\"\n",
        "\n",
        "    cls_embeddings = []\n",
        "    for _, row in df[df['label'] == label].iterrows():\n",
        "        input_text = row['text']\n",
        "\n",
        "        if isinstance(input_text, str) and input_text != 'nan':\n",
        "            cls_embeddings.extend(extract_cls_embeddings(input_text))\n",
        "        else:\n",
        "            # Handle NaN values or invalid text\n",
        "            print(f\"Skipping NaN value in row: {row.name}\")\n",
        "\n",
        "\n",
        "    return torch.mean(torch.stack(cls_embeddings), dim=0)\n",
        "\n",
        "\n",
        "\n",
        "mean_tensor_intermediate = process_texts(train_df, label=0)\n",
        "mean_tensor_expert = process_texts(train_df, label=1)\n",
        "mean_tensor_beginner = process_texts(train_df, label=2)\n",
        "\n",
        "'''\n",
        "# Optionally, print the results or return them as needed\n",
        "print(f\"Mean CLS embedding for beginners: {mean_tensor_beginner}\")\n",
        "print(f\"Mean CLS embedding for experts: {mean_tensor_expert}\")\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "Gt1Mg811qc3O",
        "outputId": "efe992d6-c8c2-4c98-de31-a9575d740b5b"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping NaN value in row: 160\n",
            "Skipping NaN value in row: 505\n",
            "Skipping NaN value in row: 656\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Optionally, print the results or return them as needed\\nprint(f\"Mean CLS embedding for beginners: {mean_tensor_beginner}\")\\nprint(f\"Mean CLS embedding for experts: {mean_tensor_expert}\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "import torch\n",
        "\n",
        "# Process test texts\n",
        "test_cls_embeddings = []\n",
        "for _, test_row in test_df.iterrows():\n",
        "    input_text = test_row['text']\n",
        "\n",
        "    if isinstance(input_text, str) and input_text != 'nan':  # Check for valid text\n",
        "\n",
        "\n",
        "        cls_embeddings = extract_cls_embeddings(input_text)\n",
        "\n",
        "        # Aggregate the embeddings (e.g., by averaging the embeddings of all chunks)\n",
        "        mean_cls_embedding = torch.mean(torch.stack(cls_embeddings), dim=0)\n",
        "\n",
        "        # Append the aggregated [CLS] embedding along with the label\n",
        "        test_cls_embeddings.append((mean_cls_embedding, test_row['label']))\n",
        "\n",
        "    else:\n",
        "        # Handle NaN or invalid text\n",
        "        print(f\"Skipping invalid input_text in test_row: {test_row[0]}\")  # Optional: Print a message for debugging\n",
        "\n",
        "# Optional: Print the results\n",
        "print(\"Aggregated CLS embeddings for the test set:\", len(test_cls_embeddings))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CIvAXSyREzR",
        "outputId": "7277cabe-3504-4e24-9c9a-6530a37aa018"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-77-cfeb62dd5cc0>:22: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  print(f\"Skipping invalid input_text in test_row: {test_row[0]}\")  # Optional: Print a message for debugging\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping invalid input_text in test_row: Portal:Tornadoes\n",
            "Aggregated CLS embeddings for the test set: 183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "correct = 0\n",
        "beginner_correct = 0\n",
        "expert_correct = 0\n",
        "intermediate_correct = 0\n",
        "\n",
        "for test_cls_embedding in test_cls_embeddings :\n",
        "\n",
        "\n",
        "  if test_cls_embedding[1] == 0 :\n",
        "    truth = \"beginner\"\n",
        "\n",
        "  elif test_cls_embedding[1] == 1:\n",
        "    truth = \"expert\"\n",
        "\n",
        "  elif test_cls_embedding[1] == 2:\n",
        "    truth = \"intermediate\"\n",
        "\n",
        "\n",
        "  #cTODO : rename refactor anch check if using the embeddings\n",
        "  cls_test_normalized = test_cls_embedding[0].cpu().detach().numpy() / np.linalg.norm(test_cls_embedding[0].cpu().detach().numpy())\n",
        "\n",
        "  cls_beginner_normalized = mean_tensor_beginner.cpu().detach().numpy() / np.linalg.norm(mean_tensor_beginner.cpu().detach().numpy())\n",
        "  similarity_beg = cosine_similarity(cls_test_normalized, cls_beginner_normalized)[0][0]\n",
        "\n",
        "  cls_expert_normalized = mean_tensor_expert.cpu().detach().numpy() / np.linalg.norm(mean_tensor_expert.cpu().detach().numpy())\n",
        "  similarity_expert = cosine_similarity(cls_test_normalized, cls_expert_normalized)[0][0]\n",
        "\n",
        "  cls_intermediate_normalized = mean_tensor_intermediate.cpu().detach().numpy() / np.linalg.norm(mean_tensor_intermediate.cpu().detach().numpy())\n",
        "  similarity_intermediate = cosine_similarity(cls_test_normalized, cls_intermediate_normalized)[0][0]\n",
        "\n",
        "\n",
        "\n",
        "  if similarity_beg > similarity_expert and similarity_beg > similarity_intermediate and truth == \"beginner\" :\n",
        "    beginner_correct += 1\n",
        "    correct += 1\n",
        "  elif similarity_expert > similarity_beg and similarity_expert > similarity_intermediate and truth == \"expert\" :\n",
        "    expert_correct += 1\n",
        "    correct += 1\n",
        "  elif similarity_intermediate > similarity_beg and similarity_intermediate > similarity_expert and truth == \"intermediate\" :\n",
        "    intermediate_correct += 1\n",
        "    correct += 1\n",
        "\n",
        "\n",
        "print(\"expert correct : \", expert_correct/len(test_df[test_df['label'] == 1]),\"\\n\")\n",
        "print(\"beginner correct : \", beginner_correct/len(test_df[test_df['label'] == 0]),\"\\n\")\n",
        "print(\"intermediate correct : \", intermediate_correct/len(test_df[test_df['label'] == 2]),\"\\n\")\n",
        "print(\"correct : \", correct/len(test_cls_embeddings),\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gF3MmXinR7Wm",
        "outputId": "7b41f0f7-8675-4639-83cf-5bfc913536d6"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "expert correct :  0.6129032258064516 \n",
            "\n",
            "beginner correct :  0.11290322580645161 \n",
            "\n",
            "intermediate correct :  0.13333333333333333 \n",
            "\n",
            "correct :  0.2896174863387978 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Initialize counters for True Positives (TP), False Positives (FP), and False Negatives (FN)\n",
        "tp_beg = 0  # True positives for beginner\n",
        "fp_beg = 0  # False positives for beginner\n",
        "fn_beg = 0  # False negatives for beginner\n",
        "tp_exp = 0  # True positives for expert\n",
        "fp_exp = 0  # False positives for expert\n",
        "fn_exp = 0  # False negatives for expert\n",
        "\n",
        "correct = 0\n",
        "beginner_correct = 0\n",
        "expert_correct = 0\n",
        "\n",
        "# Loop through test_cls_embeddings to classify and update confusion matrix\n",
        "for test_cls_embedding in test_cls_embeddings:\n",
        "    if test_cls_embedding[1] == 0:\n",
        "        truth = \"beginner\"\n",
        "    else:\n",
        "        truth = \"expert\"\n",
        "\n",
        "    # Normalize the vectors for cosine similarity\n",
        "    cls_test_normalized = test_cls_embedding[0].cpu().detach().numpy() / np.linalg.norm(test_cls_embedding[0].cpu().detach().numpy())\n",
        "\n",
        "    # Compare with beginner's mean vector\n",
        "    cls_normalized_beg = mean_tensor_beginner.cpu().detach().numpy() / np.linalg.norm(mean_tensor_beginner.cpu().detach().numpy())\n",
        "    similarity_beg = cosine_similarity(cls_test_normalized.reshape(1, -1), cls_normalized_beg.reshape(1, -1))[0][0]\n",
        "\n",
        "    # Compare with expert's mean vector\n",
        "    cls_normalized_exp = mean_tensor_expert.cpu().detach().numpy() / np.linalg.norm(mean_tensor_expert.cpu().detach().numpy())\n",
        "    similarity_expert = cosine_similarity(cls_test_normalized.reshape(1, -1), cls_normalized_exp.reshape(1, -1))[0][0]\n",
        "\n",
        "    # Predict the label based on similarity\n",
        "    predicted = \"beginner\" if similarity_beg >= similarity_expert else \"expert\"\n",
        "\n",
        "    # Update counters for confusion matrix based on prediction vs. truth\n",
        "    if predicted == \"beginner\":\n",
        "        if truth == \"beginner\":\n",
        "            tp_beg += 1  # True Positive for beginner\n",
        "        else:\n",
        "            fp_beg += 1  # False Positive for beginner\n",
        "            fn_exp += 1  # False Negative for expert\n",
        "    elif predicted == \"expert\":\n",
        "        if truth == \"expert\":\n",
        "            tp_exp += 1  # True Positive for expert\n",
        "        else:\n",
        "            fp_exp += 1  # False Positive for expert\n",
        "            fn_beg += 1  # False Negative for beginner\n",
        "\n",
        "    # Count correct predictions\n",
        "    if predicted == truth:\n",
        "        correct += 1\n",
        "        if truth == \"beginner\":\n",
        "            beginner_correct += 1\n",
        "        else:\n",
        "            expert_correct += 1\n",
        "\n",
        "# Calculate total True Positives, False Positives, and False Negatives\n",
        "tp_total = tp_beg + tp_exp\n",
        "fp_total = fp_beg + fp_exp\n",
        "fn_total = fn_beg + fn_exp\n",
        "\n",
        "# Calculate precision, recall, and F1 score for overall classification\n",
        "precision_total = tp_total / (tp_total + fp_total) if (tp_total + fp_total) > 0 else 0\n",
        "recall_total = tp_total / (tp_total + fn_total) if (tp_total + fn_total) > 0 else 0\n",
        "f1_total = 2 * (precision_total * recall_total) / (precision_total + recall_total) if (precision_total + recall_total) > 0 else 0\n",
        "\n",
        "# Print the overall F1 score\n",
        "print(f\"Overall F1 score: {f1_total:.4f}\")\n",
        "\n",
        "# Create confusion matrix\n",
        "conf_matrix = np.array([[tp_beg, fp_exp],\n",
        "                        [fn_beg, tp_exp]])\n",
        "\n",
        "# Plot the confusion matrix using seaborn's heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Beginner\", \"Expert\"], yticklabels=[\"Beginner\", \"Expert\"], cbar=False)\n",
        "\n",
        "# Set plot labels and title\n",
        "plt.title(\"Confusion Matrix\", fontsize=16)\n",
        "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
        "plt.ylabel(\"True Label\", fontsize=12)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "sN1iSXg9LPKU",
        "outputId": "840a141a-3db3-4311-996e-42c001581e24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall F1 score: 0.7353\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhQAAAHcCAYAAACOKTOhAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAPwtJREFUeJzt3Xd8FNX+//H3JpBCAqEkoZdIEVACoSgiEFApYgHxShMIcK9ErxRFBSuooBQVQbEhCooiIHLlykU6oYQqvZcYuhBqIAQC2ZzfH3yzP5YkEDKBncjr+XjsQ/bMmdnPRpZ958yZMw5jjBEAAIAFXp4uAAAA5H0ECgAAYBmBAgAAWEagAAAAlhEoAACAZQQKAABgGYECAABYRqAAAACWESgAAIBlBArgOubNm6fu3burSpUqKlSokHx9fVWyZEk1a9ZMH3/8sY4dO+bpErVt2za1adNGoaGh8vb2lsPh0Ntvv31La3A4HHI4HLf0NW9UhQoVXHX27dv3mn0/+OADV998+fLdogqzZ+/evXI4HKpQoYKnSwFcHCy9DWTu+PHj6tixo+bPny/p8pdReHi4AgICdOTIEa1atUrJyckKDAzU/Pnzde+993qkznPnzunuu+/W3r17VbduXVWtWlXe3t5q06aN2rRpc8vqSA8Tdv4npUKFCtq3b58kqVixYjp8+LB8fHwy7VutWjXt2LFDkuTt7a3U1FTLr793716FhYWpfPny2rt3r8ePA+Qme8VuwCYSExPVsGFD7dy5U1WrVtXYsWPVqFEjtz4pKSn67rvvNGjQIP31118eqlRas2aN9u7dqwYNGig2NtZjdWzfvt1jr32j6tatqz/++EMzZszQU089lWH78uXLtWPHDtWrV09r1qzxQIXXVrp0aW3fvl358+f3dCmAC6c8gEz07t1bO3fuVIUKFRQbG5shTEiSr6+vevbsqQ0bNqhatWoeqPKy/fv3S5IqV67ssRokqWrVqqpatapHa8iuHj16SJK+/fbbTLd/8803bv3sJn/+/KpataoqVqzo6VKA/88AcBMXF2e8vb2NJDN9+vQcH+enn34yDzzwgClSpIjx8fEx5cqVM927dzc7d+7MtH/58uWNJBMfH28WLlxomjVrZgoXLmz8/PxMRESE+e6779z6L1q0yEjK8pHu6udXi4yMNJLMokWL3NpPnz5t3njjDXP33XebAgUKGB8fH1OyZEnToEED89Zbb5mLFy+69b/W65w4ccK89tprpnr16sbf398EBgaa2rVrm+HDh5vk5OQM/dPfW2RkpLl48aIZNmyYqV69uvHz8zNFixY1TzzxhNm2bVuW7ykr6T/jpUuXmrp16xovLy9z8OBBtz5nz541gYGBpkyZMiYuLs5IMt7e3hmOtXXrVjNw4EDToEEDU6pUKZM/f35TtGhR8+CDD5opU6Zk6B8VFZWt/1+DBg0yksygQYPMvn37TI8ePUyZMmVMvnz5TFRUlDHGmPj4eCPJlC9f3u01evXqZSSZhg0bmkuXLmWo4fXXXzeSTEREhDl//vwN//yAa+GUB3CVmTNnyul0qnDhwnr88cdveH9jjLp166bvv/9e+fLlU+PGjRUaGqp169Zp/PjxmjJlin755Re1bNky0/2//fZbDRkyRLVr11bLli21d+9erVy5UlFRUTp58qReeOEFSVKJEiUUFRWlPXv2KDY2VhUrVlTDhg2tvHWX5ORkNWzYUFu2bFFISIgefPBB19yRHTt2aPny5erXr58KFy583WP9+eefeuCBB7Rv3z6FhISoVatWunTpkhYtWqQBAwZoypQpmj9/vooUKZJh30uXLqlVq1Zavny5GjdurGrVqmn16tX6z3/+o0WLFmn9+vU5npjYo0cP/fHHH5owYYLeeOMNV/vUqVOVlJSkvn37yssr60HckSNH6ptvvlHVqlVVo0YNFS5cWPv379eiRYu0YMECrVy5UiNHjnT1b9iwoZKSkvTLL78oICBA//jHP65Z3+7duxURESEfHx/df//9MsYoODj4mvt89NFHWrlypZYtW6Y333xTw4YNc22bPXu2hg4dqkKFCmnq1Kny8/O73o8IuDGeTjSA3XTp0sVIMg888ECO9v/iiy+MJBMcHGzWr1/vak9LS3P99lm4cGGTkJDgtl/6b8/58+c3v/32m9u28ePHG0kmKCgow2/06dvSf3u9mnIwQvHdd98ZSebhhx/OMBLhdDpNTEyMSUlJydbr3HvvvUaSefzxx01SUpKrPSEhwdSuXdtIMp06dXLb58rRl4iICPPXX3+5tp0/f960aNHCSDI9e/bM8n1l5soRitOnTxt/f39TqVIltz7333+/cTgcJi4uzjUSkNkIRUxMjImLi8vQvmPHDlOmTBkjyaxatcptW1YjC1dK/zsiyXTu3NlcuHAhQ59rHefPP/80hQsXNg6Hw8yaNcsYY8yBAwdMcHCwkWSmTp2a5WsDVjCHArhK+mWgoaGhOdr/ww8/lCQNHDhQtWrVcrU7HA4NGjRI4eHhOn36tL7++utM9+/du7ceffRRt7Zu3bqpatWqSkxM1B9//JGjum7E0aNHJUnNmjXLMPHPy8tLkZGRWV4dcaVly5Zp1apVKlCggMaOHauAgADXtpCQEI0dO1aSNHnyZB08eDDD/g6HQ+PHj1eJEiVcbX5+fnrnnXckyXUFTk4EBQWpbdu22rNnjxYvXixJ2rlzp2JjYxUZGak77rjjmvtn1efOO+/UW2+9JUmaNm1ajusrWrSoxowZI19f3xvaLywsTBMmTJAxRl26dFF8fLw6dOig48ePq1evXplOQgVyA4ECyEUHDx5UXFycJCkqKirDdofDoe7du0uSFi1alOkxHnvssUzb0yd+Hjp0KDdKvaZ69epJkkaMGKHvv/9eJ0+ezNFxYmJiJEktW7ZU8eLFM2yvU6eOatasqbS0NNeX+pXKlSunmjVrZmjPrZ/F1ZMz0/+b3cmYSUlJ+vnnn/X666+rZ8+e6tatm7p166ZffvlF0uWAklMPPfSQgoKCcrRv69at1a9fP504cUIRERGKjY1V3bp19dFHH+W4HuB6mEMBXCUkJESSlJCQcMP7pn/BFStWTIUKFcq0T/rM/Ky+DMuVK5dpe/rxLly4cMN13agmTZpowIAB+uCDDxQVFSWHw6HKlSvr/vvvV+vWrfXYY49dc35BuvT3GBYWlmWfihUrauPGjZn+PK73s0hJScnO28lS06ZNFRYWpmnTpmnUqFH6/vvvVahQoevOb5Ck3377Td27d9eJEyey7HPmzJkc12Z10arhw4dr9uzZ2rZtmwICAjR16tRsjSoBOcUIBXCVOnXqSJLWrVsnp9N5y18/O1/UuSktLS3T9mHDhikuLk6ffPKJnnrqKZ07d07jx49XmzZtVL9+fZ07d+6m13azfxYOh0PdunVTcnKyoqKidOTIEXXo0EH+/v7X3O/QoUNq3769Tpw4of79+2vjxo1KTEyU0+mUMUZz5syRZG2Rr+vVcD2rVq3Srl27JF1e/Gzz5s2WjgdcD4ECuMqjjz4qLy8vnT59Wv/9739vaN/SpUtLkk6cOJHlb6d//vmnW9+bLX0OxNmzZzPdnr5yZGYqVKig3r17a8qUKTp48KBWr16tKlWqaM2aNRoxYsR1Xzv9Paa/58zc6p/H1bp16yYvLy/99ttvkrJ3uuO3337T+fPn9cQTT2j48OEKDw9XoUKFXAFo9+7dN7Xm6zl+/Lg6dOig1NRUde/e3RWcrvX/GrCKQAFcpWLFiurYsaMk6aWXXrru/IGEhATXufIyZcq4TmlMmDAhQ19jjKu9adOmuVf0NaR/UWe2kuWmTZt04MCBbB+rXr16+ve//y1J2rBhw3X7N2nSRNLlSxbTJ3peaf369dqwYYO8vLzUuHHjbNeRm8qVK6fWrVurWLFiql+/fraWUE//O1G+fPkM24wxmjRpUqb7pZ9yyI1lvLOSPhnz4MGD6tq1q7799lu99NJLOnXqlNq3b69Lly7dtNfG7Y1AAWTi008/VaVKlRQfH6+GDRtq2bJlGfpcvHhR3377rSIiIty+rF9++WVJ0uDBg7Vx40ZXuzFGQ4YM0YYNG1S4cGE988wzN/+N6PLkPkl655133OYc7N27V1FRUZkOy//nP//RkiVLMpwOuXTpkmbPni0p8y/TqzVs2FD33nuvzp8/r+joaCUnJ7u2HT9+XNHR0ZKkDh06qGzZsjf+5nLJ9OnTdfz4ca1YsSJb/dMnhU6bNs1t2XWn06mBAwdq+fLlme4XEhIiHx8fHTlyJMcTXa9n6NChmj17tqpXr67PP//c1Xbfffdp1apV6t+//015XYBJmUAmihQpotjYWLVv314xMTFq1KiRwsLCFB4ergIFCujo0aNavXq1kpKSVKhQIZUqVcq1b3R0tJYvX66JEyeqbt26ioyMdC1stXPnTvn7+2vSpEmuyZ832+uvv65p06Zp1qxZqlKliurVq6djx45pzZo1uv/++9WgQYMMX4CLFy/W6NGjFRwcrIiICIWGhurs2bNauXKlEhISVLp06Wx/MU2aNEkPPPCAZsyYobCwMDVu3Ni1sNWZM2dUu3ZtjRkz5ma89ZvmscceU506dbR27VpVqVJFkZGRCggI0KpVq3T48GENGDBAw4cPz7Bf/vz59fjjj2vatGmqVauWGjZsqAIFCkiSxo0bZ7muJUuWaODAgSpQoIB+/vln12W6+fLl0+TJkxUREaFRo0apSZMmat26teXXA67ECAWQhdDQUC1atEi///67unbtKm9vby1YsEDTpk3Ttm3bdN9992nUqFGKj4/XPffc49rP4XDo+++/16RJk9SwYUOtXbtW06ZNU3Jysrp166b169fr4YcfvmXvIywsTMuXL1fbtm119uxZzZw5U0ePHtUbb7yhWbNmZXqDqW7duunVV19V1apVtW3bNv38889asWKFypYtq/fff18bN25UmTJlsvX6d9xxh9atW6fXXntNxYoV08yZMzVv3jxVrFhRw4YN07JlyzJdJdPO8uXLp5iYGL3++usqXbq0FixYoJiYGEVERGjFihVZroIqSV999ZWio6PlcDg0bdo0ffPNN657h1hx7NgxdezYUU6nU5999pmqV6/utr1cuXKaMGGC69Jl7lKK3MbtywEAgGWMUAAAAMsIFAAAwDICBQAAsIxAAQAALCNQAAAAywgUAADAMgIFAACw7LZYKdM/openSwBwDafW5K2VMoHbiV82kwIjFAAAwDICBQAAsIxAAQAALCNQAAAAywgUAADAMgIFAACwjEABAAAsI1AAAADLCBQAAMAyAgUAALCMQAEAACwjUAAAAMsIFAAAwDICBQAAsIxAAQAALCNQAAAAywgUAADAMgIFAACwjEABAAAsI1AAAADLCBQAAMAyAgUAALCMQAEAACwjUAAAAMsIFAAAwDICBQAAsIxAAQAALCNQAAAAywgUAADAMgIFAACwjEABAAAsI1AAAADLCBQAAMAyAgUAALCMQAEAACwjUAAAAMsIFAAAwDICBQAAsIxAAQAALCNQAAAAywgUAADAMgIFAACwjEABAAAsI1AAAADLCBQAAMAyAgUAALCMQAEAACwjUAAAAMsIFAAAwDICBQAAsIxAAQAALCNQAAAAywgUAADAMgIFAACwjEABAAAsI1AAAADLCBQAAMAyAgUAALCMQAEAACwjUAAAAMsIFAAAwDICBQAAsIxAAQAALCNQAAAAywgUAADAMgIFAACwjEABAAAss12guHTpknr06KH4+HhPlwIAALLJdoEif/78+uWXXzxdBgAAuAG2CxSS1KZNG/3666+eLgMAAGRTPk8XkJnKlSvr3XffVWxsrOrUqaOAgAC37X369PFQZQAAIDMOY4zxdBFXCwsLy3Kbw+HQn3/+eUPH84/oZbUkADfRqTVjPF0CgCz4ZXPowZYjFEzIBAAgb7HlHIp0Fy9e1M6dO5WamurpUgAAwDXYMlAkJyfrn//8pwoUKKC77rpL+/fvlyT17t1bw4YN83B1AADgarYMFK+99po2btyomJgY+fn5udofeughTZkyxYOVAQCAzNhyDsWvv/6qKVOmqH79+nI4HK72u+66S3FxcR6sDAAAZMaWIxTHjh1TaGhohvZz5865BQwAAGAPtgwUdevW1f/+9z/X8/QQMW7cON13332eKgsAAGTBlqc83n//fT388MPatm2bUlNTNXr0aG3btk3Lly/X4sWLPV0eAAC4ii1HKBo2bKgNGzYoNTVVNWrU0Ny5cxUaGqoVK1aoTp06ni4PAABcxZYrZeY2VsoE7I2VMgH7ytMrZUpSWlqa9uzZo4SEBKWlpblta9y4sYeqAgAAmbFloFi5cqU6deqkffv26eoBFIfDIafT6aHKAABAZmwZKJ599lnXlR4lS5bkUlEAAGzOloFi9+7dmjZtmipVquTpUgAAQDbY8iqPe++9V3v27PF0GQAAIJtsOULRu3dvvfTSSzpy5Ihq1Kih/Pnzu20PDw/3UGUAACAztrxs1Msr48CJw+GQMSZHkzK5bBSwNy4bBewrT182Gh8f7+kSAADADbBloChfvrynS4CNvNy9mQb3aa0xPy7SKx/+4mq/NzxMbz//qOrVqCCnM02bdh3SY//+TBdSLnmwWuD2dO5ckj77ZLQWLpivkydPqGq16ur/6uu6uwanqG8XtgwU0uUrPRYtWpTpwlYDBw70UFW41epUL6d/Pnm/Nu066NZ+b3iYZoz5tz4cP1f9hv+sVGeawquUVlqa7c7gAbeFtwe+qT27d+u9YSMUEhKq/838r6L/1V3T/ztLxYsX93R5uAVsOYfi66+/1nPPPafg4GCVKFHCbR0Kh8OhdevW3dDxmEORNwX4+2jFT6+q79ApevVfLbVp50HXCMXi717SglU79O7n/7vOUZAXMIcib7tw4YIa3FNboz79XI0jm7jaOzzVVg0bNlKvvi96rjhYlqfnUAwZMkTvvfeeBgwY4OlS4EGjXmuv2Uu3aNGqnXr1Xy1d7SFFAnVPeJgm//6HFk3op7Aywdq196jeHvOblm/404MVA7cnpzNVTqdTvr6+bu2+vr5av/7GfgFE3mXLdShOnTqlp556Kkf7pqSk6MyZM24Pk8ZS3XnNUy3qqFbVsnrr0/9m2BZWJliS9EZ0K307fblaP/+5Nmw/oFlf9VbFciG3ulTgthcQEKiatSI09svPlZBwVE6nUzN/m6FNGzfo2LEET5eHW8SWgeKpp57S3Llzc7Tv0KFDFRQU5PZIPbo2lyvEzVSmeGF98MqT6v7GBKVcTM2w3cvr8imwb35Zpon/XamNOw+q/0fTtWtvgqJa33erywUg6b2hI2SMUbOmjVUvooYm/TBRLVs9kukyAPh7suUpj0qVKumtt97SypUrM13Yqk+fPlnu+9prr6lfv35ubaGNOHWSl0RUK6fixQppxaT///8tXz5vNaxdUc+2b6zwJwZLkrb/ecRtv53xR1S2RJFbWiuAy8qWK6dvv/tBycnJOncuSSEhoXrlpRdUpkxZT5eGW8SWgWLs2LEKDAzU4sWLtXjxYrdtDofjmoHC19c3w3k8h5f3TakTN8ei1TtV5x/vubWNfaezdsYf1UcT5in+4HEdTjitKhVC3fpUKh+qubHbbmWpAK5SoEABFShQQGcSE7Uidple6PeKp0vCLWLLQMHCVre3pOQUbYv7y63t3PmLOpl4ztX+8Xfz9eazj2jzrkPauPOgOj92r+6sUFydXvnGEyUDt73YZUslY1Q+LEwH9u/Xxx+OUIWwO9T6ibaeLg23iC0DBXA9YybFyM83v0a89KSKBBXQ5l2H9OhzYxR/8LinSwNuS0lJZ/XJqJE6euSIgoIK68FmzdW774sZTlnj78s261D069dPgwcPVkBAQIY5EFcbOXLkDR2bdSgAe2MdCsC+8tw6FOvXr9elS5dcf87KlYtcAQAAe7DNCMXNxAgFYG+MUAD2ld0RCi4QBgAAltnmlMeVnnjiiUxPbTgcDvn5+alSpUrq1KmT7rzzTg9UBwAArmbLEYqgoCAtXLhQ69atk8PhkMPh0Pr167Vw4UKlpqZqypQpqlmzpmJjYz1dKgAAkE1HKEqUKKFOnTppzJgxrmVb09LS1LdvXxUsWFCTJ0/Ws88+qwEDBmjZsmUerhYAANhyUmZISIhiY2NVpUoVt/Zdu3apQYMGOn78uDZv3qxGjRrp9OnT1z0ekzIBe2NSJmBfeXpSZmpqqnbs2JGhfceOHXI6L9851M/Pj0tIAQCwCVue8ujSpYv++c9/6vXXX1e9evUkSWvWrNH777+vrl27SpIWL16su+66y5NlAgCA/2PLQPHxxx+rePHiGjFihI4ePSpJKl68uF588UUNGHD5DpTNmzdXy5YtPVkmAAD4P7acQ3GlM2fOSJIKFSqU42MwhwKwN+ZQAPaVp+dQSJfnUcyfP18//fSTa67E4cOHlZSU5OHKAADA1Wx5ymPfvn1q2bKl9u/fr5SUFDVr1kwFCxbU8OHDlZKSoi+//NLTJQIAgCvYcoSib9++qlu3rk6dOiV/f39X+xNPPKEFCxZ4sDIAAJAZW45QLF26VMuXL5ePj49be4UKFXTo0CEPVQUAALJiyxGKtLQ013oTVzp48KAKFizogYoAAMC12DJQNG/eXKNGjXI9dzgcSkpK0qBBg9SqVSvPFQYAADJly8tGDx48qBYtWsgYo927d6tu3bravXu3ihUrpqVLlyo0NPSGjsdlo4C9cdkoYF/ZvWzUloFCunzZ6OTJk7Vp0yYlJSWpdu3aevrpp90maWYXgQKwNwIFYF95eh2KEydOKF++fOrcubN69+6t4OBg7dy5U3/88YenSwMAAJmwVaDYvHmzKlSooNDQUFWtWlUbNmxQvXr19PHHH2vs2LFq2rSpfv31V0+XCQAArmKrQNG/f3/VqFFDS5YsUZMmTfToo4/qkUceUWJiok6dOqXo6GgNGzbM02UCAICr2GoORXBwsBYuXKjw8HAlJSWpUKFCWrNmjerUqSPp8u3L69evr9OnT9/QcZlDAdgbcygA+8qTcyhOnjypEiVKSJICAwMVEBCgIkWKuLYXKVJEZ8+e9VR5AAAgC7YKFJJcNwLL6jkAALAf2y293a1bN/n6+kqSLly4oGeffVYBAQGSpJSUFE+WBgAAsmCrQBEVFeX2vHPnzhn6dO3a9VaVAwAAsslWgWL8+PGeLgEAAOSA7eZQAACAvIdAAQAALCNQAAAAywgUAADAMgIFAACwjEABAAAsI1AAAADLCBQAAMAyAgUAALCMQAEAACwjUAAAAMsIFAAAwDICBQAAsIxAAQAALCNQAAAAywgUAADAMgIFAACwjEABAAAsI1AAAADLCBQAAMAyAgUAALCMQAEAACwjUAAAAMvyZadTWFiYHA7HDR3Y4XAoLi4uR0UBAIC8JVuBIjIy8oYDBQAAuH1kK1BMmDDhJpcBAADyMuZQAAAAy3IcKM6cOaNhw4apRYsWioiI0OrVqyVJJ0+e1MiRI7Vnz55cKxIAANhbtk55XO3gwYOKjIzUgQMHVLlyZe3YsUNJSUmSpKJFi+qrr77Svn37NHr06FwtFgAA2FOOAsUrr7yis2fPasOGDQoNDVVoaKjb9jZt2mjmzJm5UiAAALC/HJ3ymDt3rvr06aPq1atnevXHHXfcoQMHDlguDgAA5A05ChTnz59XSEhIltvPnj2b44IAAEDek6NAUb16dS1ZsiTL7b/++qsiIiJyXBQAAMhbchQoXnjhBU2ePFnDhw9XYmKiJCktLU179uxRly5dtGLFCr344ou5WigAALAvhzHG5GTH9957T2+//baMMUpLS5OXl5eMMfLy8tKQIUM0YMCA3K41x/wjenm6BADXcGrNGE+XACALftm8fCPHgUKS9u/fr19++UV79uxRWlqaKlasqLZt2+qOO+7I6SFvCgIFYG8ECsC+bkmgyCsIFIC9ESgA+8puoMjROhTptmzZolmzZmnv3r2SLt+VtGXLlqpRo4aVwwIAgDwmR4EiJSVF0dHRmjhxomvehHR5Yuarr76qp59+WuPGjZOPj0+uFgsAAOwpR1d5DBgwQN9//72ee+45bd++XRcuXFBKSoq2b9+uZ599Vj/88IP69++f27UCAACbytEciuDgYD3yyCP67rvvMt3epUsX/f777zp+/LjlAnMDcygAe2MOBWBf2Z1DkaMRikuXLql+/fpZbm/QoIFSU1NzcmgAAJAH5ShQtGjRQnPmzMly++zZs9W8efMcFwUAAPKWbA1knDx50u354MGD1a5dO7Vt21bPP/+8KlWqJEnavXu3PvvsM+3bt09TpkzJ/WoBAIAtZWsOhZeXV4a7iqbvllW7l5eXbU57MIcCsDfmUAD2lavrUAwcODDT25QDAABIrJQJwAYYoQDs66Ze5QEAAHAlS0tvx8bGat26dUpMTFRaWprbNofDobfeestScQAAIG/I0SmPkydP6pFHHtHq1atljJHD4XCbpJne5nQ6c73gnOCUB2BvnPIA7OumnvJ45ZVXtGnTJk2aNEl//vmnjDGaM2eOdu3apWeffVa1atXS4cOHc3JoAACQB+UoUMyaNUvR0dFq3769ChYsePlAXl6qVKmSPvvsM1WoUEEvvPBCbtYJAABsLEeB4vTp07rrrrskSYGBgZKkpKQk1/bmzZtfcyVNAADw95KjQFGqVCkdOXJEkuTr66vQ0FBt3LjRtf3QoUOsWwEAwG0kR1d5NG7cWPPmzdMbb7whSWrfvr1GjBghb29vpaWladSoUWrRokWuFgoAAOwrR4GiX79+mjdvnlJSUuTr66u3335bW7dudV0m2rhxY33yySe5WigAALCvXF0p8/Tp0/L29nZN1LQLLhsF7I3LRgH78shKmYULF1bBggU1adIkbl8OAMBt5KYsvR0fH68FCxbcjEMDAAAb4l4eAADAMgIFAACwjEABAAAss3S30byCGeSAvfWcusnTJQDIwvedwrPVL9uBIjw8eweUpISEhGz3BQAAeV+2A0XRokWzvZx2sWLFVK1atRwXBQAA8pZsB4qYmJibWAYAAMjLmJQJAAAsI1AAAADLCBQAAMAyAgUAALCMQAEAACwjUAAAAMssrZR56NAhLVmyRAkJCXryySdVpkwZOZ1OJSYmKigoSN7e3rlVJwAAsLEcjVAYY9SvXz+FhYXp6aefVr9+/bRr1y5JUlJSkipUqKBPP/00VwsFAAD2laNA8cEHH2j06NF6+eWXNW/ePBljXNuCgoLUtm1b/fLLL7lWJAAAsLccBYqvv/5aXbt21fvvv69atWpl2B4eHu4asQAAAH9/OQoUBw4cUIMGDbLcHhAQoDNnzuS4KAAAkLfkKFCEhobqwIEDWW5fu3atypUrl+OiAABA3pKjQNG2bVt9+eWX+vPPP11t6XcinTt3riZMmKCnnnoqdyoEAAC25zBXzqjMpsTERDVu3Fjx8fFq1KiRZs+erWbNmikpKUkrVqxQRESElixZogIFCtyMmm/YhVRPVwDgWnpO3eTpEgBk4ftO4dnql6MRiqCgIK1cuVL9+/fXoUOH5Ofnp8WLF+v06dMaNGiQli5dapswAQAAbr4cjVDkNYxQAPbGCAVgXzd1hAIAAOBKOVp6u0ePHtft43A49M033+Tk8AAAII/JUaBYuHCh66qOdE6nU3/99ZecTqdCQkIUEBCQKwUCAAD7y1Gg2Lt3b6btly5d0ldffaVRo0Zp3rx5VuoCAAB5SK7OocifP7969eql5s2bq1evXrl5aAAAYGM3ZVJmzZo1tWTJkptxaAAAYEM3JVDMmzePdSgAALiN5GgOxbvvvptp++nTp7VkyRKtW7dOr776qqXCAABA3pGjQPH2229n2l6kSBFVrFhRX375pZ555hkrdQEAgDwkR4EiLS0tt+sAAAB52A3PoTh//rz69eun33777WbUAwAA8qAbDhT+/v766quvdPTo0ZtRDwAAyINydJVHnTp1tGXLltyuBQAA5FE5ChSjRo3S5MmTNW7cOKWmcitPAABud9m+ffmSJUtUrVo1hYSEqEaNGjpx4oSOHj0qX19flS5dWv7+/u4Hdji0cePGm1L0jeL25YC9cftywL6ye/vybF/l0bRpU/3www/q2LGjihUrpuDgYN155505LhAAAPx9ZDtQGGOUPpgRExNzs+oBAAB50E1ZehsAANxebihQOByOm1UHAADIw24oUHTu3Fne3t7ZeuTLl6NFOAEAQB50Q9/6Dz30kKpUqXKzagEAAHnUDQWKqKgoderU6WbVAgAA8igmZQIAAMsIFAAAwDICBQAAsCzbcyjS0tJuZh0AACAPY4QCAABYRqAAAACWESgAAIBlBAoAAGAZgQIAAFhGoAAAAJYRKAAAgGUECgAAYBmBAgAAWEagAAAAlhEoAACAZQQKAABgGYECAABYRqAAAACWESgAAIBlBAoAAGAZgQIAAFhGoAAAAJYRKAAAgGUECgAAYBmBAgAAWEagAAAAlhEoAACAZQQKAABgGYECAABYRqAAAACWESgAAIBlBAoAAGAZgQIAAFhGoAAAAJYRKAAAgGW2DBRLlixRampqhvbU1FQtWbLEAxUBAIBrsWWgaNq0qU6ePJmhPTExUU2bNvVARQAA4FpsGSiMMXI4HBnaT5w4oYCAAA9UBAAAriWfpwu4Utu2bSVJDodD3bp1k6+vr2ub0+nUpk2b1KBBA0+VBwAAsmCrQBEUFCTp8ghFwYIF5e/v79rm4+Oj+vXr65lnnvFUeQAAIAu2ChTjx4+XMUaS9OmnnyowMNDDFQEAgOyw3RwKY4x+/PFH/fXXX54uBQAAZJPtAoWXl5cqV66sEydOeLoUAACQTbYLFJI0bNgwvfLKK9qyZYunSwEAANlgqzkU6bp27ark5GTVrFlTPj4+bpMzJWW6RgUAAPAcWwaKUaNGeboEAABwA2wZKKKiojxdAgAAuAG2nEMhSXFxcXrzzTfVsWNHJSQkSJJ+//13bd261cOVAQCAq9kyUCxevFg1atTQqlWrNH36dCUlJUmSNm7cqEGDBnm4OgAAcDVbnvJ49dVXNWTIEPXr108FCxZ0tT/wwAMaM2aMByuDJ507l6TPPhmthQvm6+TJE6parbr6v/q67q4R7unSgNvKo9VDVLdskEoW8tUlp9HuY+c0ZcMRHTmb4uoTGuijDhElVSUkQPm9Hdp0+Kwmrj2sMxcy3kkafw+2HKHYvHmznnjiiQztoaGhOn78uAcqgh28PfBNrVixXO8NG6Fp//lN9zW4X9H/6q6jR496ujTgtlI1NFDzd53Qu3P3aPjCP+Xt5VD/B8Lk4335po4+3g690jRMkjRswZ8aPDdO+bwcejGygjLe9hF/F7YMFIULF850pcz169erdOnSHqgInnbhwgUtmDdXL770iurUrady5cvrued7q2y58vp58iRPlwfcVj6Midey+FM6lJiiA6cv6OuVBxQc4KOwogUkSVVCAhQS4KOxKw7oYOIFHUy8oLErDyisqL+qF+eWCn9XtgwUHTp00IABA3TkyBE5HA6lpaUpNjZWL7/8srp27erp8uABTmeqnE6n2x1oJcnX11fr16/zUFUAJMk/v7ckKeni5dMZ+bwdMpJS04yrzyWnkTFSldAAT5SIW8CWgeL9999X1apVVbZsWSUlJal69epq3LixGjRooDfffPOa+6akpOjMmTNuj5SUlGvuA/sLCAhUzVoRGvvl50pIOCqn06mZv83Qpo0bdOxYgqfLA25bDkmd65TSroRzOpR4+d/auOPJSklNU/taJeTj7ZCPt0MdI0rK28uhID9bTt1DLrBloPDx8dHXX3+tuLg4zZw5Uz/88IN27NihiRMnytvb+5r7Dh06VEFBQW6PD4YPvUWV42Z6b+gIGWPUrGlj1YuooUk/TFTLVo/Iy8uWf42B20LXeqVVOshPn8Xud7WdTXFqzLJ9qlW6kMa2u1tfPXW3Cvh4K/5kssw1joW8zWHS7xduU+nlORzZm8qTkpKSYUTCePtmGCpH3pWcnKxz55IUEhKqV156QeeTkzXmi7GeLgsW9Jy6ydMlIAe61C2l2qUL6b35cTp+7lKmfQJ9vZWWZpR8KU2fPFFNs3cc16ztx25xpbDi+07Zu5LOtr/affPNN7r77rvl5+cnPz8/3X333Ro3btx19/P19VWhQoXcHoSJv5cCBQooJCRUZxITtSJ2mZo0fdDTJQG3nS51S6lOmSANW/hnlmFCkpJSnEq+lKZqxQNUyC+f1h08cwurxK1ky5NZAwcO1MiRI9W7d2/dd999kqQVK1boxRdf1P79+/Xuu+96uEJ4QuyypZIxKh8WpgP79+vjD0eoQtgdav1EW0+XBtxWouqWUv0KRTRqyV5duJTmmheRfMmpS87Lo8qN7iiiw4kpOpuSqkrBBdS5TinN2XHcba0K/L3Y8pRHSEiIPvnkE3Xs2NGt/aefflLv3r1veC0K1lH5e5gze5Y+GTVSR48cUVBQYT3YrLl6933RbfEz5E2c8shbshoCH7vigJbFn5IktatZQg3vKKJAH28dP3dJC/ec0OwdrCOUF2X3lIctA0XhwoW1Zs0aVa5c2a19165duueee3T69OkbOh6BArA3AgVgX3l6DkWXLl30xRdfZGgfO3asnn76aQ9UBAAArsWWcyiky5My586dq/r160uSVq1apf3796tr167q16+fq9/IkSM9VSIAAPg/tgwUW7ZsUe3atSVdvo25JAUHBys4OFhbtmxx9cvupaQAAODmsmWgWLRokadLAAAAN8CWcyiOHct60ZPNmzffwkoAAEB22DJQ1KhRQ//73/8ytH/44Ye65557PFARAAC4FlsGin79+unJJ5/Uc889p/Pnz+vQoUN68MEHNWLECE2axK2qAQCwG1sGiv79+2vFihVaunSpwsPDFR4eLl9fX23atElPPPGEp8sDAABXsWWgkKRKlSrp7rvv1t69e3XmzBm1b99eJUqU8HRZAAAgE7YMFLGxsQoPD9fu3bu1adMmffHFF+rdu7fat2+vU6dOebo8AABwFVsGigceeEDt27fXypUrVa1aNf3rX//S+vXrtX//ftWoUcPT5QEAgKvYch2KuXPnKjIy0q2tYsWKio2N1XvvveehqgAAQFZsNULRqlUrJSYmusLEsGHD3G4EdurUKf30008eqg4AAGTFVoFizpw5SklJcT1///33dfLkSdfz1NRU7dy50xOlAQCAa7BVoLj6Tuo2vLM6AADIhK0CBQAAyJtsFSgcDkeGO4hyR1EAAOzPVld5GGPUrVs3+fr6SpIuXLigZ599VgEBAZLkNr8CAADYh60CRVRUlNvzzp07Z+jTtWvXW1UOAADIJlsFivHjx3u6BAAAkAO2mkMBAADyJgIFAACwjEABAAAsI1AAAADLCBQAAMAyAgUAALCMQAEAACwjUAAAAMsIFAAAwDICBQAAsIxAAQAALCNQAAAAywgUAADAMgIFAACwjEABAAAsI1AAAADLCBQAAMAyAgUAALCMQAEAACwjUAAAAMsIFAAAwDICBQAAsIxAAQAALCNQAAAAywgUAADAMgIFAACwjEABAAAsI1AAAADLCBQAAMAyAgUAALCMQAEAACwjUAAAAMsIFAAAwDICBQAAsIxAAQAALCNQAAAAywgUAADAMgIFAACwjEABAAAsI1AAAADLCBQAAMAyAgUAALCMQAEAACwjUAAAAMsIFAAAwDICBQAAsIxAAQAALCNQAAAAywgUAADAMgIFAACwjEABAAAsI1AAAADLCBQAAMAyAgUAALCMQAEAACwjUAAAAMsIFAAAwDICBQAAsIxAAQAALCNQAAAAywgUAADAMgIFAACwjEABAAAsI1AAAADLCBQAAMAyAgUAALCMQAEAACwjUAAAAMsIFAAAwDICBQAAsIxAAQAALHMYY4yniwBuREpKioYOHarXXntNvr6+ni4HwBX4fN6+CBTIc86cOaOgoCAlJiaqUKFCni4HwBX4fN6+OOUBAAAsI1AAAADLCBQAAMAyAgXyHF9fXw0aNIgJX4AN8fm8fTEpEwAAWMYIBQAAsIxAAQAALCNQAAAAywgUyBOaNGmiF154wdNlAACyQKCAJd26dZPD4XA9ihUrppYtW2rTpk25+jrTp0/X4MGDc/WYADJ+htMfLVu29HRp6tatm9q0aePpMpBNBApY1rJlS/3111/666+/tGDBAuXLl0+PPvporr5G0aJFVbBgwVw9Zk5cvHjR0yUAue7Kz3D646effvJYPU6nU2lpaR57feQMgQKW+fr6qkSJEipRooRq1aqlV199VQcOHNCxY8ckSQcOHFC7du1UuHBhFS1aVK1bt9bevXtd+6empqpPnz4qXLiwihUrpgEDBigqKsrtN5OrT3lUqFBB77//vnr06KGCBQuqXLlyGjt2rGv73r175XA4NH36dDVt2lQFChRQzZo1tWLFCrfaly1bpkaNGsnf319ly5ZVnz59dO7cObfXGTx4sLp27apChQqpZ8+eufvDA2zgys9w+qNIkSKKiYmRj4+Pli5d6uo7YsQIhYaG6ujRo5IufzZ79eqlXr16KSgoSMHBwXrrrbd05YoEKSkpevnll1W6dGkFBATo3nvvVUxMjGv7hAkTVLhwYf33v/9V9erV5evrqx49eui7777TjBkzXKMmV+4D+yFQIFclJSXphx9+UKVKlVSsWDFdunRJLVq0UMGCBbV06VLFxsYqMDBQLVu2dP22P3z4cP34448aP368YmNjdebMGf3666/Xfa2PPvpIdevW1fr16/Xvf/9bzz33nHbu3OnW54033tDLL7+sDRs2qEqVKurYsaNSU1MlSXFxcWrZsqWefPJJbdq0SVOmTNGyZcvUq1cvt2N8+OGHqlmzptavX6+33nord35QQB6QHuS7dOmixMRE12dg3LhxKl68uKvfd999p3z58mn16tUaPXq0Ro4cqXHjxrm29+rVSytWrNDkyZO1adMmPfXUU2rZsqV2797t6pOcnKzhw4dr3Lhx2rp1qz755BO1a9fObfSkQYMGt/T94wYZwIKoqCjj7e1tAgICTEBAgJFkSpYsadauXWuMMWbixInmzjvvNGlpaa59UlJSjL+/v5kzZ44xxpjixYubDz74wLU9NTXVlCtXzrRu3drVFhkZafr27et6Xr58edO5c2fX87S0NBMaGmq++OILY4wx8fHxRpIZN26cq8/WrVuNJLN9+3ZjjDH//Oc/Tc+ePd3ez9KlS42Xl5c5f/6863XatGlj5UcE2NrVn+H0x3vvvWeMufx5rVWrlmnXrp2pXr26eeaZZ9z2j4yMNNWqVXP7jA8YMMBUq1bNGGPMvn37jLe3tzl06JDbfg8++KB57bXXjDHGjB8/3kgyGzZsyFDblf8OwN7yeTbO4O+gadOm+uKLLyRJp06d0ueff66HH35Yq1ev1saNG7Vnz54M8x8uXLiguLg4JSYm6ujRo7rnnntc27y9vVWnTp3rnkMNDw93/dnhcKhEiRJKSEjIsk/JkiUlSQkJCapatao2btyoTZs26ccff3T1McYoLS1N8fHxqlatmiSpbt26N/LjAPKcKz/D6YoWLSpJ8vHx0Y8//qjw8HCVL19eH3/8cYb969evL4fD4Xp+33336aOPPpLT6dTmzZvldDpVpUoVt31SUlJUrFgx13MfHx+3zyvyHgIFLAsICFClSpVcz8eNG6egoCB9/fXXSkpKUp06ddy+tNOFhIRYet38+fO7PXc4HBlCyJV90v/BS++TlJSk6Oho9enTJ8Oxy5Ur5/pzQECApToBu7v6M3y15cuXS5JOnjypkydP3tBnIikpSd7e3lq7dq28vb3dtgUGBrr+7O/v7xZKkPcQKJDrHA6HvLy8dP78edWuXVtTpkxRaGioChUqlGn/4sWLa82aNWrcuLGkyzO8161bp1q1at3UOmvXrq1t27Zd8x9S4HYXFxenF198UV9//bWmTJmiqKgozZ8/X15e/38K3qpVq9z2WblypSpXrixvb29FRETI6XQqISFBjRo1uqHX9vHxkdPpzJX3gZuPSZmwLCUlRUeOHNGRI0e0fft29e7dW0lJSXrsscf09NNPKzg4WK1bt9bSpUsVHx+vmJgY9enTRwcPHpQk9e7dW0OHDtWMGTO0c+dO9e3bV6dOnbrpv60MGDBAy5cvV69evbRhwwbt3r1bM2bMyDApE/i7u/IznP44fvy4nE6nOnfurBYtWqh79+4aP368Nm3apI8++sht//3796tfv37auXOnfvrpJ3366afq27evJKlKlSp6+umn1bVrV02fPl3x8fFavXq1hg4dqv/973/XrKtChQratGmTdu7cqePHj+vSpUs37WcA6xihgGWzZ892zU8oWLCgqlatqp9//llNmjSRJC1ZskQDBgxQ27ZtdfbsWZUuXVoPPviga8RiwIABOnLkiLp27Spvb2/17NlTLVq0yDA8mtvCw8O1ePFivfHGG2rUqJGMMapYsaLat29/U18XsJsrP8Pp7rzzTnXq1En79u3TzJkzJV2ehzR27Fh17NhRzZs3V82aNSVJXbt21fnz53XPPffI29tbffv2dbvEevz48RoyZIheeuklHTp0SMHBwapfv/5116t55plnFBMTo7p16yopKUmLFi1y/bsC++H25bCdtLQ0VatWTe3atWN1TMDmmjRpolq1amnUqFGeLgUexggFPG7fvn2aO3euIiMjlZKSojFjxig+Pl6dOnXydGkAgGxiDgU8zsvLSxMmTFC9evV0//33a/PmzZo/f77rsk0AgP1xygMAAFjGCAUAALCMQAEAACwjUAAAAMsIFAAAwDICBQAAsIxAAcClQoUK6tatm+t5TEyMHA6HYmJiPFbT1a6u8VZo0qSJ7r777lw9pifeB3AzESgAm5gwYYIcDofr4efnpypVqqhXr146evSop8u7IbNmzdLbb7/t0RocDgf3ZQFuIVbKBGzm3XffVVhYmC5cuKBly5bpiy++0KxZs7RlyxYVKFDgltbSuHFjnT9/Xj4+Pje036xZs/TZZ595PFQAuHUIFIDNPPzww6pbt64k6V//+peKFSumkSNHasaMGerYsWOm+5w7d04BAQG5XouXl5f8/Pxy/bgA/n445QHY3AMPPCBJio+PlyR169ZNgYGBiouLU6tWrVSwYEE9/fTTki7fWG3UqFG666675Ofnp+LFiys6OlqnTp1yO6YxRkOGDFGZMmVUoEABNW3aVFu3bs3w2lnNoVi1apVatWqlIkWKKCAgQOHh4Ro9erSrvs8++0yS3E7hpMvtGq2YMWOGHnnkEZUqVUq+vr6qWLGiBg8eLKfTmWn/tWvXqkGDBvL391dYWJi+/PLLDH1SUlI0aNAgVapUSb6+vipbtqz69++vlJSUXK0dsBtGKACbi4uLkyQVK1bM1ZaamqoWLVqoYcOG+vDDD12nQqKjozVhwgR1795dffr0UXx8vMaMGaP169crNjZW+fPnlyQNHDhQQ4YMUatWrdSqVSutW7dOzZs318WLF69bz7x58/Too4+qZMmS6tu3r0qUKKHt27dr5syZ6tu3r6Kjo3X48GHNmzdPEydOzLD/ragxuyZMmKDAwED169dPgYGBWrhwoQYOHKgzZ87ogw8+cOt76tQptWrVSu3atVPHjh01depUPffcc/Lx8VGPHj0kXQ5Ljz/+uJYtW6aePXuqWrVq2rx5sz7++GPt2rVLv/76a67VDtiOAWAL48ePN5LM/PnzzbFjx8yBAwfM5MmTTbFixYy/v785ePCgMcaYqKgoI8m8+uqrbvsvXbrUSDI//vijW/vs2bPd2hMSEoyPj4955JFHTFpamqvf66+/biSZqKgoV9uiRYuMJLNo0SJjjDGpqakmLCzMlC9f3pw6dcrtda481vPPP28y++flZtSYFUnm+eefv2af5OTkDG3R0dGmQIEC5sKFC662yMhII8l89NFHrraUlBRTq1YtExoaai5evGiMMWbixInGy8vLLF261O2YX375pZFkYmNjXW3ly5fP1vsA8gpOeQA289BDDykkJERly5ZVhw4dFBgYqP/85z8qXbq0W7/nnnvO7fnPP/+soKAgNWvWTMePH3c96tSpo8DAQC1atEiSNH/+fF28eFG9e/d2OxXxwgsvXLe29evXKz4+Xi+88IIKFy7stu3KY2XlVtR4I/z9/V1/Pnv2rI4fP65GjRopOTlZO3bscOubL18+RUdHu577+PgoOjpaCQkJWrt2rev9VatWTVWrVnV7f+mnrdLfH/B3xCkPwGY+++wzValSRfny5VPx4sV15513ysvLPfvny5dPZcqUcWvbvXu3EhMTFRoamulxExISJEn79u2TJFWuXNlte0hIiIoUKXLN2tJPv+R0TYZbUeON2Lp1q958800tXLhQZ86ccduWmJjo9rxUqVIZJr5WqVJFkrR3717Vr19fu3fv1vbt2xUSEpLp66W/P+DviEAB2Mw999zjusojK76+vhlCRlpamkJDQ/Xjjz9muk9WX3K3kp1qPH36tCIjI1WoUCG9++67qlixovz8/LRu3ToNGDBAaWlpN3zMtLQ01ahRQyNHjsx0e9myZa2WDdgWgQL4m6hYsaLmz5+v+++/320o/2rly5eXdHm04I477nC1Hzt2LMOVFpm9hiRt2bJFDz30UJb9sjr9cStqzK6YmBidOHFC06dPV+PGjV3t6VfTXO3w4cMZLs/dtWuXpMurXkqX39/GjRv14IMPZusUEPB3whwK4G+iXbt2cjqdGjx4cIZtqampOn36tKTLczTy58+vTz/9VMYYV59Ro0Zd9zVq166tsLAwjRo1ynW8dFceK/1L9+o+t6LG7PL29s5Q98WLF/X5559n2j81NVVfffWVW9+vvvpKISEhqlOnjqTL7+/QoUP6+uuvM+x//vx5nTt3LtfqB+yGEQrgbyIyMlLR0dEaOnSoNmzYoObNmyt//vzavXu3fv75Z40ePVr/+Mc/FBISopdffllDhw7Vo48+qlatWmn9+vX6/fffFRwcfM3X8PLy0hdffKHHHntMtWrVUvfu3VWyZEnt2LFDW7du1Zw5cyTJ9QXbp08ftWjRQt7e3urQocMtqfFKf/zxh4YMGZKhvUmTJmrQoIGKFCmiqKgo9enTRw6HQxMnTnQLGFcqVaqUhg8frr1796pKlSqaMmWKNmzYoLFjx7oude3SpYumTp2qZ599VosWLdL9998vp9OpHTt2aOrUqZozZ851T2cBeZZHrzEB4JJ+2eiaNWuu2S8qKsoEBARkuX3s2LGmTp06xt/f3xQsWNDUqFHD9O/f3xw+fNjVx+l0mnfeeceULFnS+Pv7myZNmpgtW7ZkuJTx6stG0y1btsw0a9bMFCxY0AQEBJjw8HDz6aefuranpqaa3r17m5CQEONwODJcQpqbNWZFUpaPwYMHG2OMiY2NNfXr1zf+/v6mVKlSpn///mbOnDkZ3nNkZKS56667zB9//GHuu+8+4+fnZ8qXL2/GjBmT4XUvXrxohg8fbu666y7j6+trihQpYurUqWPeeecdk5iY6OrHZaP4u3EYk0UcBwAAyCbmUAAAAMsIFAAAwDICBQAAsIxAAQAALCNQAAAAywgUAADAMgIFAACwjEABAAAsI1AAAADLCBQAAMAyAgUAALCMQAEAACz7fwEqFAQ8MU+JAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second method : BERT fine-tuned"
      ],
      "metadata": {
        "id": "GV9S9M4kYL_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the tokenizer globally\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, labels, max_length=512):\n",
        "        \"\"\"\n",
        "        Initialize the dataset for BERT processing.\n",
        "\n",
        "        :param df: pandas DataFrame containing the text and label columns\n",
        "        :param labels: dictionary or list of labels corresponding to the texts\n",
        "        :param max_length: maximum sequence length for BERT input\n",
        "        \"\"\"\n",
        "        self.max_length = max_length\n",
        "        self.labels = []\n",
        "        self.texts = []\n",
        "        self.document_ids = []  # Store the document ids to link chunks\n",
        "\n",
        "        for idx, text in enumerate(df['text']):\n",
        "            if isinstance(text, str) and text:\n",
        "                # Tokenize the text into tokens\n",
        "                tokens = tokenizer.tokenize(text)\n",
        "\n",
        "                # Manually chunk the tokenized text\n",
        "                chunks = self.chunk_tokens(tokens)\n",
        "\n",
        "                # For each chunk, encode it and append to the dataset\n",
        "                for chunk in chunks:\n",
        "                    encoded = tokenizer.encode_plus(\n",
        "                        chunk,\n",
        "                        add_special_tokens=True,  # Add [CLS] and [SEP] automatically\n",
        "                        max_length=max_length,\n",
        "                        padding='max_length',  # Pad to max_length\n",
        "                        return_tensors='pt'     # Return as PyTorch tensor\n",
        "                    )\n",
        "                    self.texts.append(encoded)\n",
        "                    self.labels.append(labels[df['label'].astype(str).iloc[idx]])  # Assuming labels is a dictionary or list\n",
        "                    self.document_ids.append(idx)  # Track the document ID\n",
        "\n",
        "        if len(self.labels) != len(self.texts):\n",
        "            raise ValueError(\"Mismatch between number of texts and labels.\")\n",
        "\n",
        "    def chunk_tokens(self, tokens, chunk_size=510):\n",
        "        \"\"\"Chunk tokenized text into smaller parts.\"\"\"\n",
        "        chunks = []\n",
        "        for i in range(0, len(tokens), chunk_size):\n",
        "            chunk = tokens[i:i + chunk_size]\n",
        "            chunks.append(chunk)\n",
        "        return chunks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        document_id = self.document_ids[idx]\n",
        "        return text['input_ids'].squeeze(0), text['attention_mask'].squeeze(0), label, document_id\n"
      ],
      "metadata": {
        "id": "J2SmFgCveive"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self, num_classes, dropout=0.1):\n",
        "        super(BertClassifier, self).__init__()\n",
        "\n",
        "        # Use BertForSequenceClassification which includes BERT model + classification head\n",
        "        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes, output_hidden_states=True)\n",
        "        self.dropout = nn.Dropout(dropout)  # Optional dropout layer\n",
        "        self.cls_embeddings = None\n",
        "        self.logits = None\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Forward pass through the BERT model and aggregate CLS embeddings by document.\n",
        "\n",
        "        :param input_ids: Tensor of input token IDs (batch_size x seq_len)\n",
        "        :param attention_mask: Tensor of attention masks (batch_size x seq_len)\n",
        "        :param document_ids: Tensor of document IDs (batch_size), linking chunks from the same document\n",
        "        :return: logits: Classification logits\n",
        "        \"\"\"\n",
        "        # Forward pass through BERT model\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True, output_attentions=True)\n",
        "\n",
        "        # Extract the last hidden state (the last layer hidden states)\n",
        "        last_hidden_state = outputs.hidden_states[-1]  # Shape: (batch_size, sequence_length, hidden_size)\n",
        "\n",
        "        # Get the CLS embedding for each chunk (first token in each sequence)\n",
        "        cls_embeddings = last_hidden_state[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
        "\n",
        "        # Apply dropout\n",
        "        cls_embeddings = self.dropout(cls_embeddings)\n",
        "\n",
        "        self.cls_embeddings = cls_embeddings\n",
        "\n",
        "        logits = self.bert.classifier(cls_embeddings)  # Apply classification head\n",
        "        self.logits = logits\n",
        "\n",
        "        # Extract attention scores from the output\n",
        "        attention_scores = outputs.attentions  # This will be a tuple of attention layers' scores\n",
        "\n",
        "        return logits #, attention_scores\n",
        "\n"
      ],
      "metadata": {
        "id": "oR9LZjNPk5aw"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the tokenizer globally\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, verbose=False):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.best_loss = float('inf')\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if val_loss < self.best_loss:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "                if self.verbose:\n",
        "                    print(f\"Early stopping triggered after {self.patience} epochs without improvement.\")\n",
        "\n",
        "def train(model, train_data, val_data, labels, learning_rate, epochs):\n",
        "    # Initialize datasets\n",
        "    train_dataset = Dataset(train_data, labels)\n",
        "    val_dataset = Dataset(val_data, labels)\n",
        "\n",
        "    def collate_fn(batch):\n",
        "\n",
        "        input_ids = pad_sequence([item[0] for item in batch], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "        attention_mask = pad_sequence([item[1] for item in batch], batch_first=True, padding_value=0)\n",
        "        labels = torch.tensor([item[2] for item in batch])\n",
        "        document_ids = torch.tensor([item[3] for item in batch])\n",
        "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'document_ids': document_ids}, labels\n",
        "\n",
        "    # DataLoader\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn, num_workers=2)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=2, collate_fn=collate_fn, num_workers=2)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "    early_stopping = EarlyStopping(patience=5, verbose=True)\n",
        "\n",
        "    # Gradient accumulation steps\n",
        "    accumulation_steps = 4  # Adjust this based on your needs\n",
        "\n",
        "    for epoch_num in range(epochs):\n",
        "\n",
        "        cls_embeddings_with_text_train = []\n",
        "        cls_embeddings_with_text_val = []\n",
        "\n",
        "        total_acc_train, total_loss_train = 0, 0\n",
        "        model.train()\n",
        "\n",
        "        for i, (train_input, train_label) in enumerate(tqdm(train_dataloader)):\n",
        "            train_label = train_label.to(device)\n",
        "            input_ids = train_input['input_ids'].to(device)\n",
        "            attention_mask = train_input['attention_mask'].to(device)\n",
        "            document_ids = train_input['document_ids'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(input_ids, attention_mask)\n",
        "\n",
        "            batch_loss = criterion(output, train_label)\n",
        "            total_loss_train += batch_loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            total_correct_train = (output.argmax(dim=1) == train_label).sum().item()\n",
        "            total_acc_train += total_correct_train\n",
        "\n",
        "            # Normalize loss to account for accumulation\n",
        "            batch_loss = batch_loss / accumulation_steps\n",
        "            batch_loss.backward()\n",
        "\n",
        "            if (i + 1) % accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            # Store CLS embeddings and text for further analysis\n",
        "            # TODO. : double check the below function\n",
        "            cls_embeddings_with_text_train.extend(zip(\n",
        "                [tokenizer.decode(ids, skip_special_tokens=True) for ids in input_ids],\n",
        "                model.cls_embeddings.detach().cpu().numpy(),\n",
        "                train_label.cpu().numpy()\n",
        "            ))\n",
        "\n",
        "        # Validation loop\n",
        "        total_acc_val, total_loss_val = 0, 0\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for val_input, val_label in val_dataloader:\n",
        "                val_label = val_label.to(device)\n",
        "                input_ids = val_input['input_ids'].to(device)\n",
        "                attention_mask = val_input['attention_mask'].to(device)\n",
        "                document_ids = val_input['document_ids'].to(device)\n",
        "\n",
        "                output = model(input_ids, attention_mask)\n",
        "                #print(val_label)\n",
        "                #print(output)\n",
        "                batch_loss = criterion(output, val_label)\n",
        "                total_loss_val += batch_loss.item()\n",
        "\n",
        "                # Calculate accuracy for validation\n",
        "                total_correct_val = (output.argmax(dim=1) == val_label).sum().item()\n",
        "                total_acc_val += total_correct_val\n",
        "\n",
        "                # Store CLS embeddings and text for validation\n",
        "                cls_embeddings_with_text_val.extend(zip(\n",
        "                    [tokenizer.decode(ids, skip_special_tokens=True) for ids in input_ids],\n",
        "                    model.cls_embeddings.detach().cpu().numpy(),\n",
        "                    val_label.cpu().numpy()\n",
        "                ))\n",
        "\n",
        "        # Calculate average losses and accuracies\n",
        "        avg_train_loss = total_loss_train / len(train_dataloader)\n",
        "        avg_train_acc = total_acc_train / (len(train_dataloader) * train_dataloader.batch_size)\n",
        "        avg_val_loss = total_loss_val / len(val_dataloader)\n",
        "        avg_val_acc = total_acc_val / (len(val_dataloader) * val_dataloader.batch_size)\n",
        "\n",
        "        print(f'Epochs: {epoch_num + 1} | Train Loss: {avg_train_loss:.3f} | Train Accuracy: {avg_train_acc:.3f} | Val Loss: {avg_val_loss:.3f} | Val Accuracy: {avg_val_acc:.3f}')\n",
        "\n",
        "        # Print which parameters were updated\n",
        "        print(\"Updated parameters during this epoch:\")\n",
        "        updated_params = []\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.grad is not None:  # Only parameters that were updated will have gradients\n",
        "                updated_params.append(name)\n",
        "        print(f\"Parameters updated: {updated_params}\")\n",
        "\n",
        "        # Check for early stopping\n",
        "        '''\n",
        "        early_stopping(avg_val_loss)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Stopping training early!\")\n",
        "            break\n",
        "        '''\n",
        "\n",
        "    return cls_embeddings_with_text_train, cls_embeddings_with_text_val\n",
        "\n"
      ],
      "metadata": {
        "id": "v0iZIW8M0cR1"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "num_classes = 3\n",
        "model = BertClassifier(num_classes=num_classes)\n",
        "LR = 1e-05\n",
        "\n",
        "\n",
        "labels = {\n",
        "    '0': 0,\n",
        "    '1': 1,\n",
        "    '2': 2\n",
        "}\n",
        "\n",
        "cls_embeddings_with_text_train,cls_embeddings_with_text_val = train(model, train_df, val_df, labels, LR, EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56_YvEmt0jnX",
        "outputId": "8c8c450d-8e45-4c5d-9251-f1a11473ad29"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|██████████| 844/844 [00:57<00:00, 14.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1 | Train Loss: 0.450 | Train Accuracy: 0.816 | Val Loss: 0.277 | Val Accuracy: 0.905\n",
            "Updated parameters during this epoch:\n",
            "Parameters updated: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 844/844 [00:57<00:00, 14.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 2 | Train Loss: 0.193 | Train Accuracy: 0.931 | Val Loss: 0.272 | Val Accuracy: 0.902\n",
            "Updated parameters during this epoch:\n",
            "Parameters updated: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 844/844 [00:57<00:00, 14.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 3 | Train Loss: 0.098 | Train Accuracy: 0.966 | Val Loss: 0.259 | Val Accuracy: 0.907\n",
            "Updated parameters during this epoch:\n",
            "Parameters updated: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 844/844 [00:57<00:00, 14.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 4 | Train Loss: 0.034 | Train Accuracy: 0.991 | Val Loss: 0.239 | Val Accuracy: 0.926\n",
            "Updated parameters during this epoch:\n",
            "Parameters updated: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 844/844 [00:57<00:00, 14.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 5 | Train Loss: 0.008 | Train Accuracy: 0.999 | Val Loss: 0.252 | Val Accuracy: 0.929\n",
            "Updated parameters during this epoch:\n",
            "Parameters updated: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 844/844 [00:57<00:00, 14.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 6 | Train Loss: 0.003 | Train Accuracy: 1.000 | Val Loss: 0.300 | Val Accuracy: 0.926\n",
            "Updated parameters during this epoch:\n",
            "Parameters updated: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 844/844 [00:57<00:00, 14.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 7 | Train Loss: 0.003 | Train Accuracy: 1.000 | Val Loss: 0.315 | Val Accuracy: 0.919\n",
            "Updated parameters during this epoch:\n",
            "Parameters updated: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 844/844 [00:57<00:00, 14.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 8 | Train Loss: 0.002 | Train Accuracy: 1.000 | Val Loss: 0.311 | Val Accuracy: 0.931\n",
            "Updated parameters during this epoch:\n",
            "Parameters updated: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 844/844 [00:57<00:00, 14.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 9 | Train Loss: 0.001 | Train Accuracy: 1.000 | Val Loss: 0.301 | Val Accuracy: 0.929\n",
            "Updated parameters during this epoch:\n",
            "Parameters updated: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 844/844 [00:57<00:00, 14.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 10 | Train Loss: 0.001 | Train Accuracy: 1.000 | Val Loss: 0.325 | Val Accuracy: 0.931\n",
            "Updated parameters during this epoch:\n",
            "Parameters updated: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 844/844 [00:57<00:00, 14.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 11 | Train Loss: 0.001 | Train Accuracy: 1.000 | Val Loss: 0.337 | Val Accuracy: 0.931\n",
            "Updated parameters during this epoch:\n",
            "Parameters updated: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 844/844 [00:57<00:00, 14.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 12 | Train Loss: 0.001 | Train Accuracy: 1.000 | Val Loss: 0.354 | Val Accuracy: 0.929\n",
            "Updated parameters during this epoch:\n",
            "Parameters updated: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 844/844 [00:57<00:00, 14.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 13 | Train Loss: 0.000 | Train Accuracy: 1.000 | Val Loss: 0.399 | Val Accuracy: 0.931\n",
            "Updated parameters during this epoch:\n",
            "Parameters updated: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 844/844 [00:57<00:00, 14.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 14 | Train Loss: 0.000 | Train Accuracy: 1.000 | Val Loss: 0.374 | Val Accuracy: 0.931\n",
            "Updated parameters during this epoch:\n",
            "Parameters updated: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 844/844 [00:57<00:00, 14.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 15 | Train Loss: 0.000 | Train Accuracy: 1.000 | Val Loss: 0.357 | Val Accuracy: 0.933\n",
            "Updated parameters during this epoch:\n",
            "Parameters updated: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 844/844 [00:57<00:00, 14.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 16 | Train Loss: 0.000 | Train Accuracy: 1.000 | Val Loss: 0.380 | Val Accuracy: 0.926\n",
            "Updated parameters during this epoch:\n",
            "Parameters updated: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 844/844 [00:57<00:00, 14.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 17 | Train Loss: 0.000 | Train Accuracy: 1.000 | Val Loss: 0.375 | Val Accuracy: 0.931\n",
            "Updated parameters during this epoch:\n",
            "Parameters updated: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 844/844 [00:57<00:00, 14.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 18 | Train Loss: 0.000 | Train Accuracy: 1.000 | Val Loss: 0.396 | Val Accuracy: 0.929\n",
            "Updated parameters during this epoch:\n",
            "Parameters updated: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 844/844 [00:57<00:00, 14.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 19 | Train Loss: 0.046 | Train Accuracy: 0.984 | Val Loss: 0.315 | Val Accuracy: 0.907\n",
            "Updated parameters during this epoch:\n",
            "Parameters updated: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 844/844 [00:57<00:00, 14.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 20 | Train Loss: 0.027 | Train Accuracy: 0.989 | Val Loss: 0.621 | Val Accuracy: 0.857\n",
            "Updated parameters during this epoch:\n",
            "Parameters updated: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm  # Optional, for progress bar\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "logit_list = []\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_testset(model, test_data, labels):\n",
        "    test = Dataset(test_data, labels)\n",
        "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=1, shuffle=False)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_acc_test = 0\n",
        "    cls_embeddings = []\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Iterate over each batch in the test set\n",
        "        for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
        "            # Extract batch data\n",
        "            input_ids, attention_mask, test_label, document_ids = batch\n",
        "\n",
        "            if input_ids is None:  # Skip any empty batches\n",
        "                continue\n",
        "\n",
        "            # Move to device\n",
        "            test_label = test_label.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            input_ids = input_ids.to(device)\n",
        "\n",
        "            # Forward pass through the model\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            #print(test_label.cpu().detach().numpy()[0])\n",
        "            result = -logits[0, 0] + logits[0, 1]\n",
        "            #print(result.cpu().detach().numpy())\n",
        "            logit_list.append((result.cpu().detach().numpy(),test_label.cpu().detach().numpy()[0]))\n",
        "\n",
        "            '''\n",
        "            logit_class_1 = logits[0, 1]\n",
        "            #print(logit_class_1)\n",
        "            sigmoid = nn.Sigmoid()\n",
        "            expertise_score = sigmoid(logit_class_1)\n",
        "            print(test_label)\n",
        "            print(logit_class_1)\n",
        "            print(expertise_score)\n",
        "            '''\n",
        "            '''\n",
        "\n",
        "            temperature = 3.0  # A temperature value > 1 softens the probabilities\n",
        "\n",
        "            logits_scaled = logits / temperature\n",
        "            sigmoid = nn.Sigmoid()\n",
        "            prob = sigmoid(logits_scaled)\n",
        "            print(test_label)\n",
        "            print(prob[0, 1])\n",
        "            '''\n",
        "\n",
        "            #print(logits)\n",
        "            # Get predicted class labels from logits\n",
        "            predicted_labels = logits.argmax(dim=1)\n",
        "\n",
        "\n",
        "            #print(predicted_labels)\n",
        "\n",
        "            # Store the CLS embeddings along with their document_id\n",
        "            for cls_embedding, doc_id in zip(model.cls_embeddings, document_ids):\n",
        "                # Append a tuple (cls_embedding, document_id) to the list\n",
        "                cls_embeddings.append((cls_embedding, doc_id.item(),-logits[0, 0] + logits[0, 1]))\n",
        "\n",
        "            # Calculate accuracy for this batch (based on the predicted labels)\n",
        "            correct_preds = (predicted_labels == test_label).sum().item()\n",
        "            total_acc_test += correct_preds\n",
        "            total_samples += test_label.size(0)  # Add number of samples in the current batch\n",
        "\n",
        "    # Calculate overall accuracy\n",
        "    accuracy = total_acc_test / total_samples\n",
        "\n",
        "    # Print the overall accuracy\n",
        "    print(f'Test Accuracy: {accuracy: .3f}')\n",
        "\n",
        "    return cls_embeddings\n"
      ],
      "metadata": {
        "id": "wfmX2qgSgtxP"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_embeddings_test = evaluate_testset(model, test_df, labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDEm1uy7g4Yb",
        "outputId": "cd9069fb-a2b0-410e-ee06-694b64e5e2b9"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 400/400 [00:04<00:00, 86.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy:  0.848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensors_list_beginner =  [torch.tensor(t[1]) for t in cls_embeddings_with_text_train if t[2].item() == 0]  # Convert to tensors\n",
        "tensors_list_expert =  [torch.tensor(t[1]) for t in cls_embeddings_with_text_train if t[2].item() == 1]  # Convert to tensors\n",
        "tensors_list_intermediate =  [torch.tensor(t[1]) for t in cls_embeddings_with_text_train if t[2].item() == 2]\n",
        "\n",
        "mean_tensor_beginner = torch.mean(torch.stack(tensors_list_beginner), dim=0)\n",
        "mean_tensor_expert = torch.mean(torch.stack(tensors_list_expert), dim=0)\n",
        "mean_tensor_intermediate = torch.mean(torch.stack(tensors_list_intermediate), dim=0)"
      ],
      "metadata": {
        "id": "OraCQsYWhj5M"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a defaultdict to accumulate vectors and count for each label\n",
        "from collections import defaultdict\n",
        "\n",
        "aggregated = defaultdict(lambda: {'sum': [], 'count': 0})\n",
        "\n",
        "# Group by label, accumulate sum of vectors, and count the occurrences\n",
        "for cls_embedding, doc_id,x in cls_embeddings_test:\n",
        "    aggregated[doc_id]['sum'].append(cls_embedding)\n",
        "    aggregated[doc_id]['count'] += 1\n",
        "\n",
        "# Calculate the mean (average) for each group\n",
        "for doc_id in aggregated:\n",
        "    aggregated[doc_id]['mean'] = torch.mean(torch.stack(aggregated[doc_id]['sum']), dim=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "2GlCGErWJ34U"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "THL9VyNvgiuA",
        "outputId": "1687b034-93b6-4480-b947-42b6d57cc5fc"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             category                                               text  \\\n",
              "1    Portal:Tornadoes                                                NaN   \n",
              "2           Petrushka  Petrushka is a neoclassical ballet with music ...   \n",
              "3            Celestia  Celestia is a free 3D astronomy program for Wi...   \n",
              "4                Lung  Lung (PSF).png\\nThe lung is an organ that allo...   \n",
              "5         LibreOffice  \\nLibreOffice's logo\\nLibreOffice is a free an...   \n",
              "..                ...                                                ...   \n",
              "180         Arabesque  Arabesque is an artistic decoration. It uses \"...   \n",
              "181          Flywheel  A flywheel is a heavy disk or wheel that is at...   \n",
              "182             Music  Music is sounds made by voices or instruments ...   \n",
              "183              Blog  A blog (a truncation of weblog) is a discussio...   \n",
              "184      Fire ecology  Fire ecology is a scientific discipline concer...   \n",
              "\n",
              "     label  \n",
              "1        1  \n",
              "2        2  \n",
              "3        0  \n",
              "4        2  \n",
              "5        2  \n",
              "..     ...  \n",
              "180      0  \n",
              "181      0  \n",
              "182      2  \n",
              "183      2  \n",
              "184      1  \n",
              "\n",
              "[184 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1e7adfcc-4fd7-40da-b52c-fc21d4725051\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Portal:Tornadoes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Petrushka</td>\n",
              "      <td>Petrushka is a neoclassical ballet with music ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Celestia</td>\n",
              "      <td>Celestia is a free 3D astronomy program for Wi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Lung</td>\n",
              "      <td>Lung (PSF).png\\nThe lung is an organ that allo...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LibreOffice</td>\n",
              "      <td>\\nLibreOffice's logo\\nLibreOffice is a free an...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>Arabesque</td>\n",
              "      <td>Arabesque is an artistic decoration. It uses \"...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181</th>\n",
              "      <td>Flywheel</td>\n",
              "      <td>A flywheel is a heavy disk or wheel that is at...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182</th>\n",
              "      <td>Music</td>\n",
              "      <td>Music is sounds made by voices or instruments ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183</th>\n",
              "      <td>Blog</td>\n",
              "      <td>A blog (a truncation of weblog) is a discussio...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184</th>\n",
              "      <td>Fire ecology</td>\n",
              "      <td>Fire ecology is a scientific discipline concer...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>184 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1e7adfcc-4fd7-40da-b52c-fc21d4725051')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1e7adfcc-4fd7-40da-b52c-fc21d4725051 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1e7adfcc-4fd7-40da-b52c-fc21d4725051');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-23329c61-635c-45fb-b6f8-aa93f3eceb67\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-23329c61-635c-45fb-b6f8-aa93f3eceb67')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-23329c61-635c-45fb-b6f8-aa93f3eceb67 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_3b73c0e8-dd6f-4ca2-9acd-350d40862f98\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('test_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_3b73c0e8-dd6f-4ca2-9acd-350d40862f98 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('test_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_df",
              "summary": "{\n  \"name\": \"test_df\",\n  \"rows\": 184,\n  \"fields\": [\n    {\n      \"column\": \"category\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 183,\n        \"samples\": [\n          \"Asepsis\",\n          \"Gas compressor\",\n          \"Draped painting\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 183,\n        \"samples\": [\n          \"A picture frame is usually a rectangular or oval shaped object designed to hold, protect and display a painting, drawing, or photograph. Most picture frames are made of wood or metal, and may have a glass cover. Some can be attached to walls, while others can stand up on a flat surface. The frames can be works of art themselves.\",\n          \"A species [1] is a kind of animal or plant. It is a basic unit of biological classification, and a formal rank in taxonomy. Originally, the word was used informally in a rather vague way, but now there are at least 26 different ways it is used.[2]\\n\\nAll animals or plants that are the same kind belonging to the same species. Wolves (Canis lupus) are one species. Humans are another species. Broadly, the idea is that (say) cats breed with cats and produce more cats. This is the basis for deciding to have a species named Felis catus. However, giving a simple definition of 'species' is rather difficult, and many people have tried.[3]\\n\\nSpecies is a word for a special kind of living thing, like a crow. Crows and ravens are similar but not the same, so they are together in a more general group called a genus. Then there is a family (like the crow family, which includes crows and ravens as well as jays and magpies), and then an order such as the songbirds, (which has many families in it, such as the crow, thrush and swallow families). The next group is the class; all birds are in the same class. After that is the phylum, such as vertebrates, which is all animals with backbones. Last of all is the kingdom, as the animal kingdom.\\n\\nThese are ways to classify living things.\\n\\nThere is a mnemonic to help people remember the order of the divisions which are listed again below: \\\"King Phillip Came Over For Great Spaghetti\\\".\",\n          \"Draped paintings are paintings on unstretched canvas or fabric that are hung, tied, or draped from individual points and allowed to bunch or fold. The style was developed in the late 1960s and 1970s by several groups of artists, and popularized most notably by American artist Sam Gilliam, who created a large number of Drape paintings throughout his career, often as large-format installation pieces designed to fill an entire wall or space.\\n\\n\\n== Historical origins ==\\nIn the late 1960s, the idea of shaped canvases in the context of contemporary art was expanding to include three-dimensional shapes and sculptural, painted reliefs. Several groups of artists working in different regions began extending this concept by experimenting with paintings without stretcher bars or made with everyday fabrics and objects, or presenting the stretcher bars themselves as art.\\nWorking in New York, Richard Tuttle began to pin colored and painted irregular geometric fabric shapes to the wall in 1967. Artists associated with the Supports/Surfaces movement in France, beginning the same year, started hanging large painted fabrics on the wall, often combining multiple pieces of fabric together. Sam Gilliam in Washington, D.C., Claude Viallat in France, and Nina Yankowitz in New York - among possibly others - concurrently and without knowledge of each other began knotting and folding their wet canvases or painted fabrics to achieve the patterns they wanted in the compositions before draping them in different combinations on the wall, starting in 1967 and 1968. Gilliam in particular rapidly increased the size of his canvases and began to suspend his works out beyond the gallery wall, sometimes tying them from points in the ceiling or middle of the room, bringing the paintings into conversation with the architectural features of the gallery space. \\nSculptors and mixed media artists including Lynda Benglis, Eva Hesse, and Robert Morris, all working around the same time in the late 1960s and early 1970s, were also beginning to use suspended and wall-based fabrics and sculptural elements, draped or shaped in similar ways. Some artists in this wave of exploration of material and form would eventually be broadly categorized by several critics and historians as early postminimalist artists, for their use of everyday materials and objects to create a new kind of \\\"anti-formalist\\\" art that rejected key aesthetic elements of minimalism and medium specificity, both of which were leading formalist strains of art production and criticism in the 1960s. Critic Robert Pincus-Witten, a leading proponent of postminimalist art, defined this as art whose content or form is an exploration of what art can or should be, but as defined through an artist or critic's own personal \\\"imperfect world of experience;\\\" this eventually also led to and encompassed the development of conceptual art.\\n\\n\\n== Origin of the term ==\\nIn 1968 and 1969 Yankowitz had two exhibitions of draped paintings in New York, using the terms \\\"Draped Paintings\\\" and \\\"Pleated Paintings\\\" as the titles of the shows. \\nCritics first described Gilliam's draped canvas paintings in 1969 variously as \\\"hanging canvas,\\\" \\\"soft\\\" paintings, and, once they became more elaborate, \\\"situations;\\\" Gilliam himself originally used the terms \\\"suspended paintings\\\" and \\\"sculptural paintings\\\" to describe the style. Several critics and art historians - and Gilliam - came to call his works in this style his Drapes or \\\"Drape paintings,\\\" both for the method of draping and for their resemblance to heavy Baroque-style fabric window drapery, which he cited as an inspiration in 1970.\\n\\n\\n== Gilliam's Drape paintings ==\\nGilliam's Drape paintings first began as an extension of an earlier series of abstract paintings displayed on beveled stretcher bars, which extended the paintings several inches off the wall like sculptured reliefs. To create these paintings he would pour and soak thinned acrylic paint onto canvases laid directly on the floor, before folding them to create clear lines and pools of color in the composition. Starting in late 1967, Gilliam experimented with draping these canvases once they had dried; he left them crumpled and folded to dry, and then used rope, leather, wire, and other materials to suspend, drape, or knot the paintings from walls and ceilings of his workshop. He tested a range of fabrics for these paintings, including linens, silks, and cotton materials.\\nThe precise genesis of Gilliam's Drape paintings is unclear, as he offered multiple explanations throughout his life. Among the most-cited origin stories is that he was inspired by laundry hanging on clotheslines in his neighborhood in such volumes that the clotheslines had to be propped up to support the weight, an explanation he told ARTnews in 1973. Alternately, he told  art historian Jonathan P. Binstock in 1994 that he had visited artist Kenneth Noland in Vermont in 1967 and engaged in a significant discussion about the sculpture of Anthony Caro and David Smith: \\\"What really shocked me is that I had never thought about sculpture at all ... And that's what led to the draped paintings; I mean, trying to produce a work that was about both painting and sculpture.\\\" Further still, he told filmmaker Rohini Tallala in 2004 that the Drape paintings had been inspired by his father's work as a hobbyist carpenter making sets for plays at their church in Louisville. In a 2011 interview with Kojo Nnamdi on WAMU, he directly refuted the widely cited origin story about laundry on clotheslines, telling Nnamdi that the Drapes were \\\"a business decision,\\\" made because he \\\"had to do something different,\\\" and that they had been inspired by Washington's Rock Creek Park.\\nGilliam's first public exhibition of these paintings in late 1968 included works like Swing. The following year, an exhibition at Washington's Corcoran Gallery of Art, Gilliam/Krebs/McGowin (with Rockne Krebs and Ed McGowin), presented ten of Gilliam's largest and most immersive Drape works up to that point. These included Baroque Cascade, a 150 ft long canvas suspended from the rafters in the Corcoran's two-story atrium gallery; and Carousel Form II, Carousel Form III, Horizontal Extension, and Light Depth, each 75 ft long and draped from the walls throughout the smaller side galleries. Baroque Cascade in particular received acclaim for combining painting and architecture to explore space, color, and shape.\\nIn the 1970s, he created larger and more immersive Drapes including metal, rocks, wooden beams, ladders, and sawhorses in the environment, sometimes draping or piling canvases over the objects instead of suspending them from above. One of his largest and most well-known works, Seahorses, was a draped installation created for the Philadelphia Museum of Art as part of a city-wide festival in 1975. Inspired by the large bronze rings that decorate the top of the museum's building, which Gilliam said had made him imagine Neptune using them to tie seahorses to his temple, the work consisted of six monumental painted canvases, two measuring 40 x 95 ft and four measuring 30 x 60 ft, hung from their respective top corners on the outside walls of the museum, attached via the rings and drooping down in upside-down arches of folds. In 1977 he reinstalled the work with five canvases instead of six, on the outside of the Brooklyn Museum.\\nGilliam integrated the natural environment into a Drape work completed in 1977 for an artist residency at the Artpark State Park in upstate New York, where he was assisted in part by his daughter Melissa. His installation Custom Road Slide, his first formal engagement with land art, was created with hundreds of yards of tobacco muslin and polypropylene stained with biodegradable pigment that he installed across the landscape of the park, overlooking the Niagara Gorge. He ran the canvases up and down hills and draped them across different parts of the park in over a dozen individual installations over the course of the summer residency, accompanied by piles of shale, pigmented sawhorses and wood boards, and detritus gathered from the park; Gilliam described the pieces as \\\"sculptural fabric placements.\\\" The form of the installation took shape after Gilliam and his assistants draped a length of painted muslin over a cliff before a storm blew it down the slope; they then reclaimed the partly destroyed muslin, draping and sculpting it with objects in various areas of the park, often running alongside the park's road.\\nAlso in 1977, as one of the first artists-in-residence at the newly established Fabric Workshop and Museum, he created Philadelphia Soft, six draped linen and canvas works that he had covered with printed designs instead of paint, repeated by hand in layered configurations using the workshop's industrial screenprinter. Critic Grace Glueck called this piece \\\"as subtle and beautiful as his abstract paintings.\\\"\\nGilliam installed a monumental Drape painting at the Whitney Museum's Phillip Morris branch gallery in 1993. The work - Golden Element Inside Gold - comprised a massive canvas draped within several large metal rings hanging from the ceiling in the gallery's large atrium.\\nIn 1997 he created an installation at the Kunstmuseum Kloster Unser Lieben Frauen in Magdeburg, Germany, inside the museum's historic chapel. Combining his earlier immersive Drape installations with his printmaking and sewing techniques, he used woodcut engravings to stamp an over 3000 ft length of polypropylene before staining, painting, and then cutting it into long, thin pieces. He sewed the strips onto a support structure in the chapel's vaulted ceiling, creating dozens of parabola forms that hung nearly to the floor in the nave of the chapel, and installed variously sized mirrors at different angles in the building's archways to reflect the painted forms. He first presented a version of this installation, created in collaboration with printmaker William Weege, in 1991 at the Walker Hill Art Center in Seoul.\\nIn 1998 at Washington's Kreeger Museum he placed several small Drape paintings in the museum's outdoor pool. The director of the museum later recalled that, after she had given Gilliam somewhat negative feedback on a work he was preparing inside a gallery, he decided to throw the canvases in the pool as a different artistic direction, building custom floatation devices to keep them situated through the exhibition. \\nGilliam was commissioned by the Art in Embassies Program in 2006 to create a permanent work for the American Embassy in Bamako, Mali. His Drape work Mali consists of several bright stained canvases draped like hoods over suspension wires in the embassy's atrium, which writer Jackson Brown has compared to Gilliam's earlier Cowl paintings.\\nIn conjunction with The Phillips Collection's 90th anniversary in 2011, Gilliam created a site-specific Drape work commissioned for the large well next to the museum's interior elliptical spiral staircase, nearly 45 years after his debut solo museum show, at The Phillips in 1967. Titled Flour Mill and directly inspired by the American abstract artist Arthur Dove's painting Flour Mill II from 1938, which is owned by the museum, Gilliam's installation comprised a series of narrow, 8\\u201310 ft long nylon panels, each folded in half over a set of wires in multiple rows spanning the width of the well next to the stairs, and hanging down from the second floor to the first. After staining and splashing the nylon with acrylic, Gilliam cut a series of rectangular openings into each panel, creating visual windows or frames that revealed the back half of the panel when folded over the wire. Later that year he staged a solo exhibition at the American University Museum, installing a series of draped canvases from the ceilings in the museum's double-height rotunda gallery, each by a single point in the canvas to form tree-like shapes that either hovered just above the floor or cascaded onto it, creating what the curator described as \\\"a forest of art.\\\" He also showed several smaller Drape constructions which he created by stitching stained and painted nylon into drooping box-like forms that he hung on the wall.\\nIn 2017 he installed Yves Klein Blue outside the main building for the show Vive Arte Viva at the 57th Vennice Biennale, similar to the presentation of the earlier Seahorses.\\n\\n\\n== References ==\\n\\n\\n=== Citations ===\\n\\n\\n=== Cited references ===\\nBeardsley, John (1991). \\\"Then and Now\\\". Sam Gilliam: Of Fireflies or Ferris Wheels, a Cultural Presentation of the United States of America (Exhibition catalogue). Washington, D.C.: Arts America Program, United States Information Agency. pp. 9\\u201318. OCLC 52922826.\\nBenson, LeGrace G. (September 1970). \\\"Sam Gilliam: Certain Attitudes\\\". Artforum. 9 (1): 56\\u201358. Archived from the original on 29 October 2023.\\nBinstock, Jonathan P., ed. (2005). Sam Gilliam: A Retrospective. Berkeley, California / Washington, D.C.: University of California Press / Corcoran Gallery of Art. ISBN 9780520246454. OCLC 58594996.\\nBinstock, Jonathan P. (2018). \\\"Selections From an Ongoing Conversation Between Sam Gilliam and Jonathan P. Binstock\\\". In Binstock, Jonathan P.; Helfenstein, Josef (eds.). The Music of Color: Sam Gilliam 1967-1973. Cologne / Basel: Walther K\\u00f6nig / Kunstmuseum Basel. pp. 11\\u201327. ISBN 9783960983408. OCLC 1059129151.\\nBinstock, Jonathan P.; Helfenstein, Josef, eds. (2018). \\\"Annotated Chronology and Exhibition History\\\". The Music of Color: Sam Gilliam 1967-1973. Cologne / Basel: Walther K\\u00f6nig / Kunstmuseum Basel. pp. 181\\u2013187. ISBN 9783960983408. OCLC 1059129151.\\nBrown, Jackson (2017). \\\"Sam Gilliam\\\". Callaloo, A Journal of African Diaspora Arts and Letters. 40 (5). Baltimore: Johns Hopkins University Press: 59\\u201368. doi:10.1353/cal.2017.0155. ISSN 1080-6512. JSTOR 26776416. S2CID 201765406 \\u2013 via JSTOR.\\nDavies, Hugh M. (1978). \\\"Outdoor Paintings\\\". Sam Gilliam: Indoor and Outdoor Paintings, 1967-1978 (Exhibition catalogue). Amherst, Massachusetts: University Gallery, University of Massachusetts/Amherst. pp. 1\\u20133. OCLC 5106592.\\nGilliam, Sam (1997). \\\"Sam Gilliam: Washington, D.C., April 15, 1997\\\". Sam Gilliam: Of Fireflies and Ferris Wheels: Monastery Parallel (Exhibition catalogue). Magdeburg: Kunstmuseum Kloster Unser Lieben Frauen Magdeburg. pp. Unpaginated. ISBN 9783930030309. OCLC 845459239.\\nHelfenstein, Josef (2018). \\\"From Object to Process: The Drapes in Context\\\". In Binstock, Jonathan P.; Helfenstein, Josef (eds.). The Music of Color: Sam Gilliam 1967-1973. Cologne / Basel: Walther K\\u00f6nig / Kunstmuseum Basel. pp. 135\\u2013150. ISBN 9783960983408. OCLC 1059129151.\\nKloner, Jay (1978). \\\"Indoor Paintings\\\". Sam Gilliam: Indoor and Outdoor Paintings, 1967-1978 (Exhibition catalogue). Amherst, Massachusetts: University Gallery, University of Massachusetts/Amherst. pp. 15\\u201319. OCLC 5106592.\\nLee, Sun-Young (Winter 1995). \\\"The Critical Writings of Robert Pincus-Witten\\\". Studies in Art Education. 36 (2). Washington, D.C.: National Art Education Association: 96\\u2013104. doi:10.2307/1320741. ISSN 2325-8039. JSTOR 1320741. OCLC 23258039. Retrieved 24 February 2024 \\u2013 via JSTOR.\\nTuchman, Phyllis (May 1977). \\\"Minimalism and Critical Response\\\". Artforum. 15 (9): 26\\u201331. Archived from the original on 4 November 2023. Retrieved 24 February 2024.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "correct = 0\n",
        "beginner_correct = 0\n",
        "expert_correct = 0\n",
        "intermediate_correct = 0\n",
        "\n",
        "\n",
        "for doc_id in aggregated.keys() :\n",
        "  print(test_df['label'][doc_id+1] )\n",
        "  test_mean_cls_embedding = aggregated[doc_id]['mean']\n",
        "\n",
        "  if test_df['label'][doc_id+1] == 0 :\n",
        "    truth = \"beginner\"\n",
        "\n",
        "  elif test_df['label'][doc_id+1] == 1 :\n",
        "    truth = \"expert\"\n",
        "\n",
        "  elif test_df['label'][doc_id+1] == 2 :\n",
        "    truth = \"intermediate\"\n",
        "\n",
        "  cls_test_normalized = test_mean_cls_embedding.cpu().detach().numpy() / np.linalg.norm(test_mean_cls_embedding.cpu().detach().numpy())\n",
        "\n",
        "  cls_beginner_normalized = mean_tensor_beginner.cpu().detach().numpy() / np.linalg.norm(mean_tensor_beginner.cpu().detach().numpy())\n",
        "  similarity_beg = cosine_similarity(cls_test_normalized.reshape(1, -1), cls_beginner_normalized.reshape(1, -1))[0][0]\n",
        "  print(similarity_beg)\n",
        "  cls_expert_normalized = mean_tensor_expert.cpu().detach().numpy() / np.linalg.norm(mean_tensor_expert.cpu().detach().numpy())\n",
        "  similarity_expert = cosine_similarity(cls_test_normalized.reshape(1, -1), cls_expert_normalized.reshape(1, -1))[0][0]\n",
        "  print(similarity_expert)\n",
        "  cls_intermediate_normalized = mean_tensor_intermediate.cpu().detach().numpy() / np.linalg.norm(mean_tensor_intermediate.cpu().detach().numpy())\n",
        "  similarity_intermediate = cosine_similarity(cls_test_normalized.reshape(1, -1), cls_intermediate_normalized.reshape(1, -1))[0][0]\n",
        "  print(similarity_intermediate,'\\n')\n",
        "\n",
        "  if similarity_beg > similarity_expert and similarity_beg > similarity_intermediate and truth == \"beginner\" :\n",
        "    beginner_correct += 1\n",
        "    correct += 1\n",
        "  elif similarity_expert > similarity_beg and similarity_expert > similarity_intermediate and truth == \"expert\" :\n",
        "    expert_correct += 1\n",
        "    correct += 1\n",
        "  elif similarity_intermediate > similarity_beg and similarity_intermediate > similarity_expert and truth == \"intermediate\" :\n",
        "    intermediate_correct += 1\n",
        "    correct += 1\n",
        "\n",
        "print(\"expert correct : \", expert_correct/len(test_df[test_df['label'] == 1]),\"\\n\")\n",
        "print(\"beginner correct : \", beginner_correct/len(test_df[test_df['label'] == 0]),\"\\n\")\n",
        "print(\"intermediate correct : \", beginner_correct/len(test_df[test_df['label'] == 2]),\"\\n\")\n",
        "print(\"correct : \", correct/len(test_df),\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZuVmHAQ1XFj",
        "outputId": "513b68cb-3473-40a8-9257-41cec8482c70"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "-0.10944211\n",
            "-0.3360289\n",
            "0.8514572 \n",
            "\n",
            "0\n",
            "0.9021743\n",
            "-0.24682474\n",
            "-0.10018643 \n",
            "\n",
            "2\n",
            "-0.27322614\n",
            "-0.31344151\n",
            "0.91587317 \n",
            "\n",
            "2\n",
            "0.5499574\n",
            "-0.41339162\n",
            "0.45870692 \n",
            "\n",
            "1\n",
            "0.53747344\n",
            "0.5783969\n",
            "-0.3448246 \n",
            "\n",
            "1\n",
            "0.16502783\n",
            "0.7079384\n",
            "-0.14468524 \n",
            "\n",
            "2\n",
            "-0.21231008\n",
            "-0.32796276\n",
            "0.94532824 \n",
            "\n",
            "1\n",
            "0.6880466\n",
            "0.032904137\n",
            "-0.080029696 \n",
            "\n",
            "1\n",
            "-0.056648433\n",
            "0.9110145\n",
            "-0.31189364 \n",
            "\n",
            "2\n",
            "-0.20017491\n",
            "-0.33155572\n",
            "0.92097205 \n",
            "\n",
            "1\n",
            "0.71690845\n",
            "0.28244084\n",
            "-0.16107002 \n",
            "\n",
            "1\n",
            "0.6752518\n",
            "0.16329074\n",
            "-0.039056662 \n",
            "\n",
            "1\n",
            "-0.14859892\n",
            "0.86398375\n",
            "-0.25328094 \n",
            "\n",
            "1\n",
            "-0.06978821\n",
            "0.87450874\n",
            "-0.3086062 \n",
            "\n",
            "0\n",
            "0.3022525\n",
            "-0.462438\n",
            "0.723746 \n",
            "\n",
            "2\n",
            "-0.047693744\n",
            "-0.36213285\n",
            "0.8923547 \n",
            "\n",
            "0\n",
            "0.32907623\n",
            "-0.39493817\n",
            "0.65854543 \n",
            "\n",
            "1\n",
            "0.17041925\n",
            "0.7778545\n",
            "-0.32059282 \n",
            "\n",
            "0\n",
            "0.83678424\n",
            "-0.24442267\n",
            "-0.036326848 \n",
            "\n",
            "0\n",
            "0.113718\n",
            "-0.42514643\n",
            "0.85176206 \n",
            "\n",
            "0\n",
            "0.89824146\n",
            "-0.27672434\n",
            "-0.10719666 \n",
            "\n",
            "0\n",
            "0.9037055\n",
            "-0.28736877\n",
            "-0.093698636 \n",
            "\n",
            "0\n",
            "0.57167315\n",
            "-0.40187034\n",
            "0.46702117 \n",
            "\n",
            "2\n",
            "-0.14752147\n",
            "-0.32401824\n",
            "0.862486 \n",
            "\n",
            "2\n",
            "-0.32098585\n",
            "-0.3148269\n",
            "0.89393985 \n",
            "\n",
            "2\n",
            "-0.25146145\n",
            "-0.3163364\n",
            "0.9294504 \n",
            "\n",
            "1\n",
            "0.11769244\n",
            "0.7908565\n",
            "-0.25064144 \n",
            "\n",
            "1\n",
            "0.4238941\n",
            "0.6127189\n",
            "-0.1471507 \n",
            "\n",
            "0\n",
            "0.72498834\n",
            "0.14904332\n",
            "-0.020378143 \n",
            "\n",
            "2\n",
            "0.31398493\n",
            "-0.4030646\n",
            "0.7164735 \n",
            "\n",
            "1\n",
            "0.8133594\n",
            "0.10732214\n",
            "-0.26594734 \n",
            "\n",
            "2\n",
            "-0.057688527\n",
            "-0.37822938\n",
            "0.9177207 \n",
            "\n",
            "2\n",
            "-0.12085888\n",
            "-0.29897436\n",
            "0.9378005 \n",
            "\n",
            "1\n",
            "-0.001537045\n",
            "0.8674035\n",
            "-0.22493693 \n",
            "\n",
            "2\n",
            "-0.23301752\n",
            "-0.3316626\n",
            "0.91463083 \n",
            "\n",
            "1\n",
            "0.13861097\n",
            "0.68644327\n",
            "-0.045087233 \n",
            "\n",
            "2\n",
            "-0.18228465\n",
            "-0.28741688\n",
            "0.9257081 \n",
            "\n",
            "2\n",
            "-0.25053132\n",
            "-0.30308884\n",
            "0.937526 \n",
            "\n",
            "2\n",
            "-0.116983436\n",
            "-0.3530616\n",
            "0.91890764 \n",
            "\n",
            "2\n",
            "0.271142\n",
            "-0.26968747\n",
            "0.59994906 \n",
            "\n",
            "1\n",
            "0.026674345\n",
            "0.89921147\n",
            "-0.28973126 \n",
            "\n",
            "0\n",
            "0.77232236\n",
            "0.21550062\n",
            "-0.26025656 \n",
            "\n",
            "2\n",
            "-0.18445411\n",
            "-0.34791303\n",
            "0.95541245 \n",
            "\n",
            "0\n",
            "0.91332614\n",
            "-0.26640314\n",
            "-0.03447975 \n",
            "\n",
            "0\n",
            "0.87658906\n",
            "-0.058902673\n",
            "-0.12485984 \n",
            "\n",
            "0\n",
            "0.69493157\n",
            "-0.14347836\n",
            "0.15303148 \n",
            "\n",
            "0\n",
            "-0.008801669\n",
            "-0.34945273\n",
            "0.80322945 \n",
            "\n",
            "1\n",
            "0.16081491\n",
            "0.78656566\n",
            "-0.18568847 \n",
            "\n",
            "2\n",
            "-0.071201354\n",
            "-0.41394156\n",
            "0.88747 \n",
            "\n",
            "1\n",
            "0.04615946\n",
            "0.8001977\n",
            "-0.24716233 \n",
            "\n",
            "2\n",
            "-0.25604296\n",
            "-0.30740052\n",
            "0.90478957 \n",
            "\n",
            "0\n",
            "0.8532979\n",
            "-0.28326815\n",
            "-0.0030014091 \n",
            "\n",
            "0\n",
            "0.8942915\n",
            "-0.29853332\n",
            "-0.10003056 \n",
            "\n",
            "1\n",
            "0.5087311\n",
            "0.1975364\n",
            "0.05170921 \n",
            "\n",
            "0\n",
            "0.84932554\n",
            "-0.2924152\n",
            "-0.011630248 \n",
            "\n",
            "1\n",
            "0.112396955\n",
            "0.85423464\n",
            "-0.31721726 \n",
            "\n",
            "2\n",
            "-0.2168786\n",
            "-0.34481966\n",
            "0.9465111 \n",
            "\n",
            "0\n",
            "0.4062891\n",
            "0.44281402\n",
            "-0.08658191 \n",
            "\n",
            "2\n",
            "-0.29644465\n",
            "-0.2718802\n",
            "0.91165966 \n",
            "\n",
            "0\n",
            "0.87965137\n",
            "-0.2964723\n",
            "-0.09632334 \n",
            "\n",
            "2\n",
            "0.541381\n",
            "-0.3924535\n",
            "0.53994983 \n",
            "\n",
            "2\n",
            "-0.27288526\n",
            "-0.31920457\n",
            "0.9191236 \n",
            "\n",
            "0\n",
            "0.90859914\n",
            "-0.22304271\n",
            "-0.028036244 \n",
            "\n",
            "2\n",
            "-0.28273618\n",
            "-0.30890623\n",
            "0.86267006 \n",
            "\n",
            "1\n",
            "-0.11969815\n",
            "0.841923\n",
            "-0.2625085 \n",
            "\n",
            "1\n",
            "0.033164963\n",
            "0.908975\n",
            "-0.366633 \n",
            "\n",
            "1\n",
            "0.49401692\n",
            "0.52733004\n",
            "-0.29583392 \n",
            "\n",
            "0\n",
            "0.87900877\n",
            "-0.3163084\n",
            "-0.033512272 \n",
            "\n",
            "0\n",
            "0.9245528\n",
            "-0.31432235\n",
            "0.04236608 \n",
            "\n",
            "0\n",
            "0.886739\n",
            "-0.29595155\n",
            "0.04289969 \n",
            "\n",
            "1\n",
            "0.63374937\n",
            "-0.015708156\n",
            "0.078667 \n",
            "\n",
            "0\n",
            "-0.06823976\n",
            "-0.3494072\n",
            "0.91085607 \n",
            "\n",
            "2\n",
            "-0.1741856\n",
            "-0.28616622\n",
            "0.8591311 \n",
            "\n",
            "1\n",
            "-0.18241294\n",
            "0.8696331\n",
            "-0.28611928 \n",
            "\n",
            "2\n",
            "0.43766844\n",
            "-0.44068325\n",
            "0.5842711 \n",
            "\n",
            "1\n",
            "0.14106645\n",
            "0.8454351\n",
            "-0.28555354 \n",
            "\n",
            "2\n",
            "-0.26476014\n",
            "-0.3483246\n",
            "0.9274516 \n",
            "\n",
            "0\n",
            "0.89098066\n",
            "-0.2950353\n",
            "-0.08793684 \n",
            "\n",
            "0\n",
            "0.90423435\n",
            "-0.2399531\n",
            "-0.037299905 \n",
            "\n",
            "2\n",
            "-0.26969814\n",
            "-0.33720136\n",
            "0.9350585 \n",
            "\n",
            "2\n",
            "0.101582795\n",
            "-0.22927892\n",
            "0.8147497 \n",
            "\n",
            "0\n",
            "0.8398993\n",
            "-0.27222842\n",
            "0.22196627 \n",
            "\n",
            "1\n",
            "0.076844074\n",
            "0.9034576\n",
            "-0.27414793 \n",
            "\n",
            "0\n",
            "-0.2514313\n",
            "-0.31539497\n",
            "0.932335 \n",
            "\n",
            "1\n",
            "-0.17572021\n",
            "0.83064985\n",
            "-0.2908305 \n",
            "\n",
            "0\n",
            "0.8767798\n",
            "-0.30513602\n",
            "-0.013164586 \n",
            "\n",
            "0\n",
            "0.79766625\n",
            "-0.28909752\n",
            "-0.012187833 \n",
            "\n",
            "0\n",
            "0.82116234\n",
            "-0.113760866\n",
            "0.016227799 \n",
            "\n",
            "0\n",
            "0.8947829\n",
            "-0.31688914\n",
            "-0.05996204 \n",
            "\n",
            "2\n",
            "-0.2775734\n",
            "-0.26950938\n",
            "0.90561956 \n",
            "\n",
            "1\n",
            "-0.13140583\n",
            "0.8698697\n",
            "-0.27632493 \n",
            "\n",
            "2\n",
            "-0.3298443\n",
            "-0.31208524\n",
            "0.900941 \n",
            "\n",
            "1\n",
            "0.065981925\n",
            "0.88440824\n",
            "-0.2963491 \n",
            "\n",
            "1\n",
            "0.0246621\n",
            "0.8587997\n",
            "-0.27338034 \n",
            "\n",
            "0\n",
            "0.9084089\n",
            "-0.31837028\n",
            "-0.05506998 \n",
            "\n",
            "0\n",
            "0.877128\n",
            "-0.3287415\n",
            "-0.05873037 \n",
            "\n",
            "1\n",
            "-0.13732797\n",
            "0.8764131\n",
            "-0.27939415 \n",
            "\n",
            "2\n",
            "-0.17449614\n",
            "-0.35165307\n",
            "0.9367187 \n",
            "\n",
            "1\n",
            "0.16906872\n",
            "0.8629628\n",
            "-0.29618725 \n",
            "\n",
            "1\n",
            "0.33510542\n",
            "0.70897275\n",
            "-0.25438625 \n",
            "\n",
            "0\n",
            "0.7704689\n",
            "-0.024036638\n",
            "-0.046419226 \n",
            "\n",
            "0\n",
            "0.8175695\n",
            "-0.35208863\n",
            "0.11902869 \n",
            "\n",
            "1\n",
            "-0.14215714\n",
            "0.84331\n",
            "-0.19064765 \n",
            "\n",
            "2\n",
            "-0.24945895\n",
            "-0.29205492\n",
            "0.9288975 \n",
            "\n",
            "2\n",
            "-0.29174596\n",
            "-0.30744708\n",
            "0.91113114 \n",
            "\n",
            "1\n",
            "0.30903035\n",
            "0.33260572\n",
            "0.04200622 \n",
            "\n",
            "2\n",
            "0.04531651\n",
            "-0.078477636\n",
            "0.63366497 \n",
            "\n",
            "2\n",
            "-0.111638814\n",
            "-0.38314837\n",
            "0.88518476 \n",
            "\n",
            "1\n",
            "0.020204052\n",
            "0.8392732\n",
            "-0.309543 \n",
            "\n",
            "2\n",
            "-0.19641116\n",
            "-0.34494737\n",
            "0.9286423 \n",
            "\n",
            "1\n",
            "0.23966949\n",
            "0.66741776\n",
            "-0.10321185 \n",
            "\n",
            "2\n",
            "-0.2161898\n",
            "-0.31688976\n",
            "0.95394135 \n",
            "\n",
            "0\n",
            "0.9068714\n",
            "-0.22597219\n",
            "-0.13232382 \n",
            "\n",
            "1\n",
            "-0.05991027\n",
            "0.90058374\n",
            "-0.2885115 \n",
            "\n",
            "0\n",
            "0.721684\n",
            "-0.03133052\n",
            "-0.034289166 \n",
            "\n",
            "2\n",
            "-0.10014439\n",
            "-0.3973846\n",
            "0.91023374 \n",
            "\n",
            "0\n",
            "0.8262604\n",
            "-0.16193597\n",
            "-0.21061006 \n",
            "\n",
            "1\n",
            "-0.111646995\n",
            "0.93050003\n",
            "-0.27422583 \n",
            "\n",
            "0\n",
            "0.79340196\n",
            "-0.02885285\n",
            "-0.10274562 \n",
            "\n",
            "0\n",
            "0.88823485\n",
            "-0.33004868\n",
            "-0.048544884 \n",
            "\n",
            "0\n",
            "0.8584188\n",
            "-0.30498672\n",
            "0.058452714 \n",
            "\n",
            "1\n",
            "0.33771095\n",
            "0.6356142\n",
            "-0.36574787 \n",
            "\n",
            "1\n",
            "-0.17540985\n",
            "0.82632273\n",
            "-0.26925135 \n",
            "\n",
            "2\n",
            "-0.3023497\n",
            "-0.2971539\n",
            "0.9079533 \n",
            "\n",
            "1\n",
            "-0.06709283\n",
            "0.87315917\n",
            "-0.27457458 \n",
            "\n",
            "2\n",
            "-0.17021689\n",
            "-0.34888893\n",
            "0.932836 \n",
            "\n",
            "1\n",
            "0.022777168\n",
            "0.82974494\n",
            "-0.31305677 \n",
            "\n",
            "2\n",
            "-0.08569673\n",
            "-0.39279222\n",
            "0.90172493 \n",
            "\n",
            "2\n",
            "0.10675712\n",
            "-0.37134963\n",
            "0.85747695 \n",
            "\n",
            "0\n",
            "0.91291344\n",
            "-0.2484134\n",
            "-0.0897038 \n",
            "\n",
            "0\n",
            "0.87484455\n",
            "-0.32451215\n",
            "0.01756877 \n",
            "\n",
            "2\n",
            "-0.29067242\n",
            "-0.3061779\n",
            "0.9151219 \n",
            "\n",
            "0\n",
            "0.75656414\n",
            "-0.25929117\n",
            "0.09378864 \n",
            "\n",
            "0\n",
            "0.23308894\n",
            "-0.44265622\n",
            "0.7679694 \n",
            "\n",
            "1\n",
            "0.5553041\n",
            "0.2958111\n",
            "-0.1475138 \n",
            "\n",
            "2\n",
            "-0.319641\n",
            "-0.2598394\n",
            "0.8941741 \n",
            "\n",
            "2\n",
            "-0.20884421\n",
            "-0.3230749\n",
            "0.9369737 \n",
            "\n",
            "1\n",
            "0.2796129\n",
            "0.78115666\n",
            "-0.23578562 \n",
            "\n",
            "2\n",
            "-0.11708345\n",
            "-0.37456492\n",
            "0.92924595 \n",
            "\n",
            "1\n",
            "0.25601095\n",
            "0.7552326\n",
            "-0.2938381 \n",
            "\n",
            "2\n",
            "-0.067380264\n",
            "-0.40986526\n",
            "0.9046525 \n",
            "\n",
            "0\n",
            "0.25018612\n",
            "-0.27006704\n",
            "0.71049434 \n",
            "\n",
            "1\n",
            "-0.057735614\n",
            "0.8772967\n",
            "-0.23367748 \n",
            "\n",
            "0\n",
            "0.8748055\n",
            "-0.33154285\n",
            "0.009793207 \n",
            "\n",
            "0\n",
            "0.91968393\n",
            "-0.12552917\n",
            "-0.12674084 \n",
            "\n",
            "2\n",
            "-0.24542111\n",
            "-0.31665784\n",
            "0.9154546 \n",
            "\n",
            "0\n",
            "0.8653637\n",
            "-0.35660827\n",
            "0.06929891 \n",
            "\n",
            "1\n",
            "0.025509363\n",
            "0.7598996\n",
            "-0.14698982 \n",
            "\n",
            "0\n",
            "0.88998413\n",
            "-0.29061258\n",
            "-0.09461184 \n",
            "\n",
            "2\n",
            "-0.13754155\n",
            "-0.3511964\n",
            "0.9111384 \n",
            "\n",
            "2\n",
            "-0.2975868\n",
            "-0.31372347\n",
            "0.90487325 \n",
            "\n",
            "1\n",
            "0.5904302\n",
            "0.4413547\n",
            "-0.17650768 \n",
            "\n",
            "1\n",
            "-0.013182372\n",
            "0.89007545\n",
            "-0.32175452 \n",
            "\n",
            "2\n",
            "-0.27142504\n",
            "-0.33145583\n",
            "0.93172634 \n",
            "\n",
            "1\n",
            "0.3487892\n",
            "0.6725457\n",
            "-0.16363107 \n",
            "\n",
            "1\n",
            "0.042142276\n",
            "0.85928273\n",
            "-0.30279467 \n",
            "\n",
            "0\n",
            "0.8832615\n",
            "-0.27187008\n",
            "-0.04744451 \n",
            "\n",
            "2\n",
            "-0.09309381\n",
            "-0.37151062\n",
            "0.9319011 \n",
            "\n",
            "0\n",
            "0.09992922\n",
            "-0.48086417\n",
            "0.75942767 \n",
            "\n",
            "0\n",
            "0.8012209\n",
            "0.20945513\n",
            "-0.17592844 \n",
            "\n",
            "1\n",
            "-0.15934767\n",
            "0.910492\n",
            "-0.36367357 \n",
            "\n",
            "2\n",
            "-0.19142422\n",
            "-0.36000973\n",
            "0.9303168 \n",
            "\n",
            "2\n",
            "-0.2664916\n",
            "-0.29803252\n",
            "0.93226373 \n",
            "\n",
            "1\n",
            "0.06415258\n",
            "0.8554524\n",
            "-0.37779015 \n",
            "\n",
            "0\n",
            "0.78398895\n",
            "-0.40607467\n",
            "0.2726524 \n",
            "\n",
            "2\n",
            "0.52590775\n",
            "-0.4001962\n",
            "0.5626918 \n",
            "\n",
            "1\n",
            "0.71825385\n",
            "0.113780364\n",
            "-0.15018025 \n",
            "\n",
            "1\n",
            "0.25448877\n",
            "0.5335255\n",
            "-0.12735613 \n",
            "\n",
            "1\n",
            "0.6644132\n",
            "0.22765072\n",
            "-0.15927224 \n",
            "\n",
            "0\n",
            "0.84572417\n",
            "-0.33581072\n",
            "0.12644562 \n",
            "\n",
            "2\n",
            "0.029496355\n",
            "-0.32174972\n",
            "0.8547901 \n",
            "\n",
            "0\n",
            "0.77781487\n",
            "0.33476835\n",
            "-0.23724034 \n",
            "\n",
            "0\n",
            "0.5419197\n",
            "-0.39135313\n",
            "0.44838303 \n",
            "\n",
            "1\n",
            "0.22957227\n",
            "0.68738973\n",
            "-0.10131037 \n",
            "\n",
            "1\n",
            "0.16699699\n",
            "0.8183194\n",
            "-0.28982723 \n",
            "\n",
            "0\n",
            "0.8891546\n",
            "-0.3125626\n",
            "-0.07792202 \n",
            "\n",
            "0\n",
            "0.835168\n",
            "-0.30219156\n",
            "0.0718274 \n",
            "\n",
            "1\n",
            "0.20682243\n",
            "0.64942145\n",
            "-0.17850721 \n",
            "\n",
            "0\n",
            "0.7995211\n",
            "-0.2540542\n",
            "0.1523898 \n",
            "\n",
            "0\n",
            "0.85305923\n",
            "-0.17277846\n",
            "0.10535686 \n",
            "\n",
            "2\n",
            "-0.2360501\n",
            "-0.32350758\n",
            "0.9288794 \n",
            "\n",
            "2\n",
            "-0.23812759\n",
            "-0.28425542\n",
            "0.881631 \n",
            "\n",
            "1\n",
            "-0.11139494\n",
            "0.9232992\n",
            "-0.31777117 \n",
            "\n",
            "expert correct :  0.8225806451612904 \n",
            "\n",
            "beginner correct :  0.8387096774193549 \n",
            "\n",
            "intermediate correct :  0.8666666666666667 \n",
            "\n",
            "correct :  0.875 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Initialize counters for the confusion matrix components\n",
        "tp_beg = 0  # True positives for beginner\n",
        "fp_beg = 0  # False positives for beginner\n",
        "fn_beg = 0  # False negatives for beginner\n",
        "tp_exp = 0  # True positives for expert\n",
        "fp_exp = 0  # False positives for expert\n",
        "fn_exp = 0  # False negatives for expert\n",
        "\n",
        "# Iterate over the aggregated dictionary (you can adjust based on your data structure)\n",
        "for doc_id in aggregated.keys():\n",
        "    test_cls_embedding = aggregated[doc_id]['mean']\n",
        "\n",
        "    # Determine the ground truth label (beginner or expert)\n",
        "    truth = \"beginner\" if test_df['label'][doc_id] == 0 else \"expert\"\n",
        "\n",
        "    # Normalize vectors\n",
        "    cls_test_normalized = test_cls_embedding.cpu().detach().numpy() / np.linalg.norm(test_cls_embedding.cpu().detach().numpy())\n",
        "    cls_normalized_beg = mean_tensor_beginner.cpu().detach().numpy() / np.linalg.norm(mean_tensor_beginner.cpu().detach().numpy())\n",
        "    cls_normalized_exp = mean_tensor_expert.cpu().detach().numpy() / np.linalg.norm(mean_tensor_expert.cpu().detach().numpy())\n",
        "\n",
        "    # Compute cosine similarities\n",
        "    similarity_beg = cosine_similarity(cls_test_normalized.reshape(1, -1), cls_normalized_beg.reshape(1, -1))[0][0]\n",
        "    similarity_exp = cosine_similarity(cls_test_normalized.reshape(1, -1), cls_normalized_exp.reshape(1, -1))[0][0]\n",
        "\n",
        "    # Classify based on similarities\n",
        "    predicted = \"beginner\" if similarity_beg > similarity_exp else \"expert\"\n",
        "\n",
        "    # Update confusion matrix counters based on prediction and truth\n",
        "    if predicted == \"beginner\":\n",
        "        if truth == \"beginner\":\n",
        "            tp_beg += 1\n",
        "        else:\n",
        "            fp_beg += 1\n",
        "    else:  # predicted == \"expert\"\n",
        "        if truth == \"expert\":\n",
        "            tp_exp += 1\n",
        "        else:\n",
        "            fp_exp += 1\n",
        "\n",
        "    if predicted != truth:  # False negatives\n",
        "        if truth == \"beginner\":\n",
        "            fn_beg += 1\n",
        "        else:\n",
        "            fn_exp += 1\n",
        "\n",
        "tp_total = tp_beg + tp_exp\n",
        "fp_total = fp_beg + fp_exp\n",
        "fn_total = fn_beg + fn_exp\n",
        "\n",
        "# Calculate precision, recall, and F1 score for overall classification\n",
        "precision_total = tp_total / (tp_total + fp_total) if (tp_total + fp_total) > 0 else 0\n",
        "recall_total = tp_total / (tp_total + fn_total) if (tp_total + fn_total) > 0 else 0\n",
        "f1_total = 2 * (precision_total * recall_total) / (precision_total + recall_total) if (precision_total + recall_total) > 0 else 0\n",
        "\n",
        "# Print the overall F1 score\n",
        "print(f\"Overall F1 score: {f1_total:.4f}\")\n",
        "\n",
        "# Create confusion matrix\n",
        "conf_matrix = np.array([[tp_beg, fp_exp],\n",
        "                        [fn_beg, tp_exp]])\n",
        "\n",
        "# Plot the confusion matrix using seaborn's heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Beginner\", \"Expert\"], yticklabels=[\"Beginner\", \"Expert\"], cbar=False)\n",
        "\n",
        "# Set plot labels and title\n",
        "plt.title(\"Confusion Matrix\", fontsize=16)\n",
        "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
        "plt.ylabel(\"True Label\", fontsize=12)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "JJauXfF3HuON",
        "outputId": "acecb2be-9519-474a-ca00-87e0dbdf6a60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall F1 score: 0.9412\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logit_list[0][0].item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C68NnaE7u5nD",
        "outputId": "fd3d4658-79ef-43f2-b4ea-2cbadd05c46b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-9.65433120727539"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_0 = []\n",
        "label_1 = []\n",
        "for item in logit_list :\n",
        "  if item[1] == 0 :\n",
        "    label_0.append((item[0].item(), item[1]))\n",
        "  else :\n",
        "    label_1.append((item[0].item(), item[1] ))\n"
      ],
      "metadata": {
        "id": "f0eAu7ExvYVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Given scores\n",
        "scores_label_1 = [ i[0] for i in label_1 ]\n",
        "scores_label_0 = [ i[0] for i in label_0 ]\n",
        "\n",
        "labels_label_1 = [ i[1] for i in label_1 ]\n",
        "labels_label_0 = [ i[1] for i in label_0 ]\n",
        "\n",
        "# Combine both sets of scores\n",
        "scores = scores_label_1 + scores_label_0\n",
        "labels = labels_label_1 + labels_label_0\n",
        "\n",
        "# Normalize the scores to range 0-1\n",
        "min_score = np.min(scores)\n",
        "max_score = np.max(scores)\n",
        "\n",
        "normalized_scores = [(score - min_score) / (max_score - min_score) for score in scores]\n",
        "\n",
        "# Scale the normalized scores to a 0-10 range\n",
        "scaled_scores = [normalized * 10 for normalized in normalized_scores]\n",
        "\n",
        "for i in range(0,len(scaled_scores)):\n",
        "  print(scaled_scores[i])\n",
        "  print(labels[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZCYL6nPlnOJ",
        "outputId": "bcaaf0f9-7fcd-460b-aa83-f65a9c4674a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9.250679525640312\n",
            "1\n",
            "9.531748090343415\n",
            "1\n",
            "9.27044655436525\n",
            "1\n",
            "9.225536090139974\n",
            "1\n",
            "9.528429236709488\n",
            "1\n",
            "9.27334936031875\n",
            "1\n",
            "9.717929200481127\n",
            "1\n",
            "9.633504188130138\n",
            "1\n",
            "9.500655218383518\n",
            "1\n",
            "9.779411247615382\n",
            "1\n",
            "8.575033972312095\n",
            "1\n",
            "9.445292293458529\n",
            "1\n",
            "9.856478522525308\n",
            "1\n",
            "9.77557290151244\n",
            "1\n",
            "9.770413819534124\n",
            "1\n",
            "9.897479862634395\n",
            "1\n",
            "9.580267325833798\n",
            "1\n",
            "9.647233307878958\n",
            "1\n",
            "9.912706209586492\n",
            "1\n",
            "9.926577792772232\n",
            "1\n",
            "9.988039423171896\n",
            "1\n",
            "9.874127655315432\n",
            "1\n",
            "9.944754584725207\n",
            "1\n",
            "9.437197738751365\n",
            "1\n",
            "5.1706032550717556\n",
            "1\n",
            "7.4177272517537185\n",
            "1\n",
            "9.755035481335634\n",
            "1\n",
            "9.871806045739929\n",
            "1\n",
            "9.463535326372618\n",
            "1\n",
            "9.786840216774907\n",
            "1\n",
            "9.513937438497424\n",
            "1\n",
            "8.421733559526555\n",
            "1\n",
            "9.809188828162462\n",
            "1\n",
            "9.450664616887007\n",
            "1\n",
            "7.300594176884145\n",
            "1\n",
            "9.226823705534791\n",
            "1\n",
            "8.670924115398469\n",
            "1\n",
            "9.078389508012503\n",
            "1\n",
            "9.36296884365468\n",
            "1\n",
            "9.310225612630209\n",
            "1\n",
            "9.825813494581375\n",
            "1\n",
            "1.6154312493004432\n",
            "1\n",
            "9.602220306275518\n",
            "1\n",
            "9.722514799070067\n",
            "1\n",
            "9.547101474755184\n",
            "1\n",
            "9.642890441579034\n",
            "1\n",
            "9.940233412276529\n",
            "1\n",
            "9.468188527038313\n",
            "1\n",
            "9.388972957944295\n",
            "1\n",
            "9.7420912716055\n",
            "1\n",
            "9.30537913354373\n",
            "1\n",
            "7.945231247422103\n",
            "1\n",
            "9.419866199610421\n",
            "1\n",
            "9.656159957946972\n",
            "1\n",
            "9.281471591043914\n",
            "1\n",
            "4.643457927552624\n",
            "1\n",
            "8.659333308319052\n",
            "1\n",
            "8.51873187759596\n",
            "1\n",
            "7.53079059086166\n",
            "1\n",
            "9.682321961650748\n",
            "1\n",
            "2.883004896885736\n",
            "1\n",
            "9.996186153978567\n",
            "1\n",
            "10.0\n",
            "1\n",
            "8.247582443114554\n",
            "1\n",
            "8.72636870726182\n",
            "1\n",
            "7.43391454634719\n",
            "1\n",
            "5.701322659585477\n",
            "1\n",
            "7.434341029247516\n",
            "1\n",
            "8.430924719733813\n",
            "1\n",
            "8.773398882242763\n",
            "1\n",
            "8.426644691605905\n",
            "1\n",
            "8.945628557077228\n",
            "1\n",
            "8.2227359576762\n",
            "1\n",
            "7.823616926162607\n",
            "1\n",
            "9.390599491133306\n",
            "1\n",
            "9.663776307359512\n",
            "1\n",
            "8.451623885819094\n",
            "1\n",
            "9.474478242407708\n",
            "1\n",
            "9.07143420709547\n",
            "1\n",
            "9.461207818629344\n",
            "1\n",
            "9.00172603074225\n",
            "1\n",
            "9.414447598250204\n",
            "1\n",
            "9.49355337068265\n",
            "1\n",
            "6.7491044879929865\n",
            "1\n",
            "9.4257090153449\n",
            "1\n",
            "9.632604490692533\n",
            "1\n",
            "8.554819590247027\n",
            "1\n",
            "7.360641154429322\n",
            "1\n",
            "8.70714793961347\n",
            "1\n",
            "8.468207491916221\n",
            "1\n",
            "9.296200677082439\n",
            "1\n",
            "9.398936778129485\n",
            "1\n",
            "2.517848422804984\n",
            "1\n",
            "1.9701488738378738\n",
            "1\n",
            "3.0573402176142612\n",
            "1\n",
            "4.2978993494003355\n",
            "1\n",
            "4.639080820687397\n",
            "1\n",
            "9.053827722595377\n",
            "1\n",
            "7.590116630823198\n",
            "1\n",
            "8.298838428450438\n",
            "1\n",
            "8.478101668351199\n",
            "1\n",
            "9.374806920075244\n",
            "1\n",
            "7.6515995853678795\n",
            "1\n",
            "9.192316701846213\n",
            "1\n",
            "7.7260473932298925\n",
            "1\n",
            "9.030264088647101\n",
            "1\n",
            "7.926493222120507\n",
            "1\n",
            "5.765089622886125\n",
            "1\n",
            "9.531833840628694\n",
            "1\n",
            "9.773111097026085\n",
            "1\n",
            "9.770965978778483\n",
            "1\n",
            "8.480854751584372\n",
            "1\n",
            "9.1904029732573\n",
            "1\n",
            "8.359264930406384\n",
            "1\n",
            "9.664928264895607\n",
            "1\n",
            "9.493533407653272\n",
            "1\n",
            "9.580083121517273\n",
            "1\n",
            "8.482456330986665\n",
            "1\n",
            "9.025108182605278\n",
            "1\n",
            "9.693542998981478\n",
            "1\n",
            "9.279985252765753\n",
            "1\n",
            "8.775318055294235\n",
            "1\n",
            "9.251931298323292\n",
            "1\n",
            "8.449524591798017\n",
            "1\n",
            "8.833257572124497\n",
            "1\n",
            "9.613344250690641\n",
            "1\n",
            "9.42150951989232\n",
            "1\n",
            "8.717836327024003\n",
            "1\n",
            "7.952486901190219\n",
            "1\n",
            "9.587209923004865\n",
            "1\n",
            "9.436818441193203\n",
            "1\n",
            "9.332534297959011\n",
            "1\n",
            "9.702936965418997\n",
            "1\n",
            "9.343837002228101\n",
            "1\n",
            "9.674123962154997\n",
            "1\n",
            "9.856811542151732\n",
            "1\n",
            "9.84655598951451\n",
            "1\n",
            "9.829608284983859\n",
            "1\n",
            "9.497254244106019\n",
            "1\n",
            "9.672580910725197\n",
            "1\n",
            "9.340485935524043\n",
            "1\n",
            "9.116331967574778\n",
            "1\n",
            "8.907303624020297\n",
            "1\n",
            "9.722568336285216\n",
            "1\n",
            "9.310328150008374\n",
            "1\n",
            "9.812389718440981\n",
            "1\n",
            "9.768091302548195\n",
            "1\n",
            "9.19925929901728\n",
            "1\n",
            "8.352333222160432\n",
            "1\n",
            "4.869709463316413\n",
            "1\n",
            "8.443324710060818\n",
            "1\n",
            "9.238076502230438\n",
            "1\n",
            "9.219156994843594\n",
            "1\n",
            "9.420474164595994\n",
            "1\n",
            "8.551642519492196\n",
            "1\n",
            "9.078374082035257\n",
            "1\n",
            "9.501543119485582\n",
            "1\n",
            "9.468200323373853\n",
            "1\n",
            "9.716776789239818\n",
            "1\n",
            "9.653868746620747\n",
            "1\n",
            "9.713422546599269\n",
            "1\n",
            "7.259875041417615\n",
            "1\n",
            "9.555220983249068\n",
            "1\n",
            "9.245222359336978\n",
            "1\n",
            "9.178350294270933\n",
            "1\n",
            "6.8941623247455865\n",
            "1\n",
            "6.272489997274593\n",
            "1\n",
            "8.033488707708266\n",
            "1\n",
            "9.669252982987008\n",
            "1\n",
            "9.009327407882756\n",
            "1\n",
            "9.364915692724152\n",
            "1\n",
            "8.7286826038487\n",
            "1\n",
            "8.444495496363258\n",
            "1\n",
            "9.42783054092142\n",
            "1\n",
            "9.603230254079909\n",
            "1\n",
            "8.923923299681867\n",
            "1\n",
            "9.41211283122352\n",
            "1\n",
            "9.94710523143435\n",
            "1\n",
            "9.791338703962928\n",
            "1\n",
            "9.168987179787905\n",
            "1\n",
            "0.2835135820965745\n",
            "0\n",
            "0.5432049182733993\n",
            "0\n",
            "0.24651619049321521\n",
            "0\n",
            "0.24475036980377657\n",
            "0\n",
            "4.260843770103395\n",
            "0\n",
            "0.04851333732261204\n",
            "0\n",
            "0.5453799810650666\n",
            "0\n",
            "0.8918297355039256\n",
            "0\n",
            "0.3656523738786846\n",
            "0\n",
            "3.082063181808345\n",
            "0\n",
            "0.4863665439953635\n",
            "0\n",
            "0.14683216441478403\n",
            "0\n",
            "2.522095557304942\n",
            "0\n",
            "0.19509687128060973\n",
            "0\n",
            "0.05780476638196815\n",
            "0\n",
            "0.19678737690467152\n",
            "0\n",
            "0.06996361238820306\n",
            "0\n",
            "0.11623110890590213\n",
            "0\n",
            "0.12100045810615369\n",
            "0\n",
            "0.1953641036511337\n",
            "0\n",
            "2.0330163098403715\n",
            "0\n",
            "0.25495646857277\n",
            "0\n",
            "0.18612167475479388\n",
            "0\n",
            "1.2902294174021267\n",
            "0\n",
            "0.43785592890403013\n",
            "0\n",
            "0.21016124547162496\n",
            "0\n",
            "0.8329351692000584\n",
            "0\n",
            "0.16652433177955933\n",
            "0\n",
            "0.09712104532955172\n",
            "0\n",
            "0.15688082747482696\n",
            "0\n",
            "0.11373709134941594\n",
            "0\n",
            "0.09414972988886991\n",
            "0\n",
            "3.3130012903829966\n",
            "0\n",
            "7.5253007577829845\n",
            "0\n",
            "8.888837821846568\n",
            "0\n",
            "8.729301457759387\n",
            "0\n",
            "9.141784830030456\n",
            "0\n",
            "0.8833309294518792\n",
            "0\n",
            "0.05021291705093607\n",
            "0\n",
            "2.5491803974121923\n",
            "0\n",
            "5.17524281622448\n",
            "0\n",
            "4.443476848761405\n",
            "0\n",
            "6.335385109295091\n",
            "0\n",
            "7.1404251208772775\n",
            "0\n",
            "4.5049940296931\n",
            "0\n",
            "5.459736089659046\n",
            "0\n",
            "6.44227646953644\n",
            "0\n",
            "8.514168283709855\n",
            "0\n",
            "0.24372227379086087\n",
            "0\n",
            "0.6631505117590637\n",
            "0\n",
            "1.979526280035115\n",
            "0\n",
            "0.42506053448379666\n",
            "0\n",
            "0.24924250511881563\n",
            "0\n",
            "0.0\n",
            "0\n",
            "0.4007419350609006\n",
            "0\n",
            "2.7442505000852284\n",
            "0\n",
            "1.5168147915884687\n",
            "0\n",
            "0.07746245215054683\n",
            "0\n",
            "0.11946965671710795\n",
            "0\n",
            "0.12439326568981818\n",
            "0\n",
            "0.5237023996877056\n",
            "0\n",
            "0.30575602646426214\n",
            "0\n",
            "0.25738288405250226\n",
            "0\n",
            "0.05147603236424471\n",
            "0\n",
            "5.77439727190685\n",
            "0\n",
            "0.11531553178583862\n",
            "0\n",
            "0.06475507654165742\n",
            "0\n",
            "0.22233778598117132\n",
            "0\n",
            "0.9929347663098289\n",
            "0\n",
            "3.197137002141897\n",
            "0\n",
            "2.044915863317312\n",
            "0\n",
            "0.47839993415829946\n",
            "0\n",
            "0.15977183709278767\n",
            "0\n",
            "0.01702165848038789\n",
            "0\n",
            "0.06522556884765636\n",
            "0\n",
            "0.13909104606863754\n",
            "0\n",
            "1.6259984974190753\n",
            "0\n",
            "0.2766635407889834\n",
            "0\n",
            "0.23786811542605438\n",
            "0\n",
            "0.20961816033152783\n",
            "0\n",
            "0.2968643116976592\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO : evaluer la distribution des scores a l'aide des graphes  entre ceux obtenus par notre modele\n",
        "# score par doc et non par chunk - verifier l'equilibre des donnees\n",
        "# le score d'attention d'un token est-il multi dimensionnel (a cause de plusieurs tetes d'attention)\n",
        "#ground truth sur 20 textes - demander a des personnes de donner des scores de 1 a 10 sur la complexite - 2 personnes pour voir si les deux eprsonnes s'entendent"
      ],
      "metadata": {
        "id": "Tkx2h96hMKuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The original list\n",
        "original_list = [\n",
        "    9.250679525640312, 1, 9.531748090343415, 1, 9.27044655436525, 1, 9.225536090139974, 1,\n",
        "    9.528429236709488, 1, 9.27334936031875, 1, 9.717929200481127, 1, 9.633504188130138, 1,\n",
        "    9.500655218383518, 1, 9.779411247615382, 1, 8.575033972312095, 1, 9.445292293458529, 1,\n",
        "    9.856478522525308, 1, 9.77557290151244, 1, 9.770413819534124, 1, 9.897479862634395, 1,\n",
        "    9.580267325833798, 1, 9.647233307878958, 1, 9.912706209586492, 1, 9.926577792772232, 1,\n",
        "    9.988039423171896, 1, 9.874127655315432, 1, 9.944754584725207, 1, 9.437197738751365, 1,\n",
        "    5.1706032550717556, 1, 7.4177272517537185, 1, 9.755035481335634, 1, 9.871806045739929, 1,\n",
        "    9.463535326372618, 1, 9.786840216774907, 1, 9.513937438497424, 1, 8.421733559526555, 1,\n",
        "    9.809188828162462, 1, 9.450664616887007, 1, 7.300594176884145, 1, 9.226823705534791, 1,\n",
        "    8.670924115398469, 1, 9.078389508012503, 1, 9.36296884365468, 1, 9.310225612630209, 1,\n",
        "    9.825813494581375, 1, 1.6154312493004432, 1, 9.602220306275518, 1, 9.722514799070067, 1,\n",
        "    9.547101474755184, 1, 9.642890441579034, 1, 9.940233412276529, 1, 9.468188527038313, 1,\n",
        "    9.388972957944295, 1, 9.7420912716055, 1, 9.30537913354373, 1, 7.945231247422103, 1,\n",
        "    9.419866199610421, 1, 9.656159957946972, 1, 9.281471591043914, 1, 4.643457927552624, 1,\n",
        "    8.659333308319052, 1, 8.51873187759596, 1, 7.53079059086166, 1, 9.682321961650748, 1,\n",
        "    2.883004896885736, 1, 9.996186153978567, 1, 10.0, 1, 8.247582443114554, 1, 8.72636870726182, 1,\n",
        "    7.43391454634719, 1, 5.701322659585477, 1, 7.434341029247516, 1, 8.430924719733813, 1,\n",
        "    8.773398882242763, 1, 8.426644691605905, 1, 8.945628557077228, 1, 8.2227359576762, 1,\n",
        "    7.823616926162607, 1, 9.390599491133306, 1, 9.663776307359512, 1, 8.451623885819094, 1,\n",
        "    9.474478242407708, 1, 9.07143420709547, 1, 9.461207818629344, 1, 9.00172603074225, 1,\n",
        "    9.414447598250204, 1, 9.49355337068265, 1, 6.7491044879929865, 1, 9.4257090153449, 1,\n",
        "    9.632604490692533, 1, 8.554819590247027, 1, 7.360641154429322, 1, 8.70714793961347, 1,\n",
        "    8.468207491916221, 1, 9.296200677082439, 1, 9.398936778129485, 1, 2.517848422804984, 1,\n",
        "    1.9701488738378738, 1, 3.0573402176142612, 1, 4.2978993494003355, 1, 4.639080820687397, 1,\n",
        "    9.053827722595377, 1, 7.590116630823198, 1, 8.298838428450438, 1, 8.478101668351199, 1,\n",
        "    9.374806920075244, 1, 7.6515995853678795, 1, 9.192316701846213, 1, 7.7260473932298925, 1,\n",
        "    9.030264088647101, 1, 7.926493222120507, 1, 5.765089622886125, 1, 9.531833840628694, 1,\n",
        "    9.773111097026085, 1, 9.770965978778483, 1, 8.480854751584372, 1, 9.1904029732573, 1,\n",
        "    8.359264930406384, 1, 9.664928264895607, 1, 9.493533407653272, 1, 9.580083121517273, 1,\n",
        "    8.482456330986665, 1, 9.025108182605278, 1, 9.693542998981478, 1, 9.279985252765753, 1,\n",
        "    8.775318055294235, 1, 9.251931298323292, 1, 8.449524591798017, 1, 8.833257572124497, 1,\n",
        "    9.613344250690641, 1, 9.42150951989232, 1, 8.717836327024003, 1, 7.952486901190219, 1,\n",
        "    9.587209923004865, 1, 9.436818441193203, 1, 9.332534297959011, 1, 9.702936965418997, 1,\n",
        "    9.343837002228101, 1, 9.674123962154997, 1, 9.856811542151732, 1, 9.84655598951451, 1,\n",
        "    9.829608284983859, 1, 9.497254244106019, 1, 9.672580910725197, 1, 9.340485935524043, 1,\n",
        "    9.116331967574778, 1, 8.907303624020297, 1, 9.722568336285216, 1, 9.310328150008374, 1,\n",
        "    9.812389718440981, 1, 9.768091302548195, 1, 9.19925929901728, 1, 8.352333222160432, 1,\n",
        "    4.869709463316413, 1, 8.443324710060818, 1, 9.238076502230438, 1, 9.219156994843594, 1,\n",
        "    9.420474164595994, 1, 8.551642519492196, 1, 9.078374082035257, 1, 9.501543119485582, 1,\n",
        "    9.468200323373853, 1, 9.716776789239818, 1, 9.653868746620747, 1, 9.713422546599269, 1,\n",
        "    7.259875041417615, 1, 9.555220983249068, 1, 9.245222359336978, 1, 9.178350294270933, 1,\n",
        "    6.8941623247455865, 1, 6.272489997274593, 1, 8.033488707708266, 1, 9.669252982987008, 1,\n",
        "    9.009327407882756, 1, 9.364915692724152, 1, 8.7286826038487, 1, 8.444495496363258, 1,\n",
        "    9.42783054092142, 1, 9.603230254079909, 1, 8.923923299681867, 1, 9.41211283122352, 1,\n",
        "    9.94710523143435, 1, 9.791338703962928, 1, 9.168987179787905, 1, 0.2835135820965745, 0,\n",
        "    0.5432049182733993, 0, 0.24651619049321521, 0, 0.24475036980377657, 0, 4.260843770103395,\n",
        "    0, 0.04851333732261204, 0, 0.5453799810650666, 0, 0.8918297355039256, 0, 0.3656523738786846,\n",
        "    0, 3.082063181808345, 0, 0.4863665439953635, 0, 0.14683216441478403, 0, 2.522095557304942,\n",
        "    0, 0.19509687128060973, 0, 0.05780476638196815, 0, 0.19678737690467152, 0, 0.06996361238820306,\n",
        "    0, 0.11623110890590213, 0, 0.12100045810615369, 0, 0.1953641036511337, 0, 2.0330163098403715,\n",
        "    0, 0.25495646857277, 0, 0.18612167475479388, 0, 1.2902294174021267, 0, 0.43785592890403013,\n",
        "    0, 0.21016124547162496, 0, 0.8329351692000584, 0, 0.16652433177955933, 0, 0.09712104532955172,\n",
        "    0, 0.15688082747482696, 0, 0.11373709134941594, 0, 0.09414972988886991, 0, 3.3130012903829966,\n",
        "    0, 7.5253007577829845, 0, 8.888837821846568, 0, 8.729301457759387, 0, 9.141784830030456,\n",
        "    0, 0.8833309294518792, 0, 0.05021291705093607, 0, 2.5491803974121923, 0, 5.17524281622448,\n",
        "    0, 4.443476848761405, 0, 6.335385109295091, 0, 7.1404251208772775, 0, 4.5049940296931,\n",
        "    0, 5.459736089659046, 0, 6.44227646953644, 0, 8.514168283709855, 0, 0.24372227379086087,\n",
        "    0, 0.6631505117590637, 0, 1.979526280035115, 0, 0.42506053448379666, 0, 0.24924250511881563,\n",
        "    0, 0.0, 0, 0.4007419350609006, 0, 2.7442505000852284, 0, 1.5168147915884687, 0,\n",
        "    0.07746245215054683, 0, 0.11946965671710795, 0, 0.12439326568981818, 0, 0.5237023996877056,\n",
        "    0, 0.30575602646426214, 0, 0.25738288405250226, 0, 0.05147603236424471, 0, 5.77439727190685,\n",
        "    0, 0.11531553178583862, 0, 0.06475507654165742, 0, 0.22233778598117132, 0, 0.9929347663098289,\n",
        "    0, 3.197137002141897, 0, 2.044915863317312, 0, 0.47839993415829946, 0, 0.15977183709278767,\n",
        "    0, 0.01702165848038789, 0, 0.06522556884765636, 0, 0.13909104606863754, 0, 1.6259984974190753,\n",
        "    0, 0.2766635407889834, 0, 0.23786811542605438, 0, 0.20961816033152783, 0, 0.2968643116976592, 0\n",
        "]\n",
        "\n",
        "# Skipping every second element\n",
        "result = original_list[::2]\n",
        "\n",
        "# Output the result\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3jaJznsN3TW",
        "outputId": "4612174f-a7b6-4f57-d800-3dee51b16a1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9.250679525640312, 9.531748090343415, 9.27044655436525, 9.225536090139974, 9.528429236709488, 9.27334936031875, 9.717929200481127, 9.633504188130138, 9.500655218383518, 9.779411247615382, 8.575033972312095, 9.445292293458529, 9.856478522525308, 9.77557290151244, 9.770413819534124, 9.897479862634395, 9.580267325833798, 9.647233307878958, 9.912706209586492, 9.926577792772232, 9.988039423171896, 9.874127655315432, 9.944754584725207, 9.437197738751365, 5.1706032550717556, 7.4177272517537185, 9.755035481335634, 9.871806045739929, 9.463535326372618, 9.786840216774907, 9.513937438497424, 8.421733559526555, 9.809188828162462, 9.450664616887007, 7.300594176884145, 9.226823705534791, 8.670924115398469, 9.078389508012503, 9.36296884365468, 9.310225612630209, 9.825813494581375, 1.6154312493004432, 9.602220306275518, 9.722514799070067, 9.547101474755184, 9.642890441579034, 9.940233412276529, 9.468188527038313, 9.388972957944295, 9.7420912716055, 9.30537913354373, 7.945231247422103, 9.419866199610421, 9.656159957946972, 9.281471591043914, 4.643457927552624, 8.659333308319052, 8.51873187759596, 7.53079059086166, 9.682321961650748, 2.883004896885736, 9.996186153978567, 10.0, 8.247582443114554, 8.72636870726182, 7.43391454634719, 5.701322659585477, 7.434341029247516, 8.430924719733813, 8.773398882242763, 8.426644691605905, 8.945628557077228, 8.2227359576762, 7.823616926162607, 9.390599491133306, 9.663776307359512, 8.451623885819094, 9.474478242407708, 9.07143420709547, 9.461207818629344, 9.00172603074225, 9.414447598250204, 9.49355337068265, 6.7491044879929865, 9.4257090153449, 9.632604490692533, 8.554819590247027, 7.360641154429322, 8.70714793961347, 8.468207491916221, 9.296200677082439, 9.398936778129485, 2.517848422804984, 1.9701488738378738, 3.0573402176142612, 4.2978993494003355, 4.639080820687397, 9.053827722595377, 7.590116630823198, 8.298838428450438, 8.478101668351199, 9.374806920075244, 7.6515995853678795, 9.192316701846213, 7.7260473932298925, 9.030264088647101, 7.926493222120507, 5.765089622886125, 9.531833840628694, 9.773111097026085, 9.770965978778483, 8.480854751584372, 9.1904029732573, 8.359264930406384, 9.664928264895607, 9.493533407653272, 9.580083121517273, 8.482456330986665, 9.025108182605278, 9.693542998981478, 9.279985252765753, 8.775318055294235, 9.251931298323292, 8.449524591798017, 8.833257572124497, 9.613344250690641, 9.42150951989232, 8.717836327024003, 7.952486901190219, 9.587209923004865, 9.436818441193203, 9.332534297959011, 9.702936965418997, 9.343837002228101, 9.674123962154997, 9.856811542151732, 9.84655598951451, 9.829608284983859, 9.497254244106019, 9.672580910725197, 9.340485935524043, 9.116331967574778, 8.907303624020297, 9.722568336285216, 9.310328150008374, 9.812389718440981, 9.768091302548195, 9.19925929901728, 8.352333222160432, 4.869709463316413, 8.443324710060818, 9.238076502230438, 9.219156994843594, 9.420474164595994, 8.551642519492196, 9.078374082035257, 9.501543119485582, 9.468200323373853, 9.716776789239818, 9.653868746620747, 9.713422546599269, 7.259875041417615, 9.555220983249068, 9.245222359336978, 9.178350294270933, 6.8941623247455865, 6.272489997274593, 8.033488707708266, 9.669252982987008, 9.009327407882756, 9.364915692724152, 8.7286826038487, 8.444495496363258, 9.42783054092142, 9.603230254079909, 8.923923299681867, 9.41211283122352, 9.94710523143435, 9.791338703962928, 9.168987179787905, 0.2835135820965745, 0.5432049182733993, 0.24651619049321521, 0.24475036980377657, 4.260843770103395, 0.04851333732261204, 0.5453799810650666, 0.8918297355039256, 0.3656523738786846, 3.082063181808345, 0.4863665439953635, 0.14683216441478403, 2.522095557304942, 0.19509687128060973, 0.05780476638196815, 0.19678737690467152, 0.06996361238820306, 0.11623110890590213, 0.12100045810615369, 0.1953641036511337, 2.0330163098403715, 0.25495646857277, 0.18612167475479388, 1.2902294174021267, 0.43785592890403013, 0.21016124547162496, 0.8329351692000584, 0.16652433177955933, 0.09712104532955172, 0.15688082747482696, 0.11373709134941594, 0.09414972988886991, 3.3130012903829966, 7.5253007577829845, 8.888837821846568, 8.729301457759387, 9.141784830030456, 0.8833309294518792, 0.05021291705093607, 2.5491803974121923, 5.17524281622448, 4.443476848761405, 6.335385109295091, 7.1404251208772775, 4.5049940296931, 5.459736089659046, 6.44227646953644, 8.514168283709855, 0.24372227379086087, 0.6631505117590637, 1.979526280035115, 0.42506053448379666, 0.24924250511881563, 0.0, 0.4007419350609006, 2.7442505000852284, 1.5168147915884687, 0.07746245215054683, 0.11946965671710795, 0.12439326568981818, 0.5237023996877056, 0.30575602646426214, 0.25738288405250226, 0.05147603236424471, 5.77439727190685, 0.11531553178583862, 0.06475507654165742, 0.22233778598117132, 0.9929347663098289, 3.197137002141897, 2.044915863317312, 0.47839993415829946, 0.15977183709278767, 0.01702165848038789, 0.06522556884765636, 0.13909104606863754, 1.6259984974190753, 0.2766635407889834, 0.23786811542605438, 0.20961816033152783, 0.2968643116976592]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " sum(result) / len(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JU32fDU5OU2_",
        "outputId": "e9a59822-e76c-4585-a950-b2dce5eab43d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.568465974495334"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the histogram\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(result, bins=20, edgecolor='black')\n",
        "\n",
        "# Adding title and labels\n",
        "plt.title('Distribution of Result List')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "t3f8YQbCON6O",
        "outputId": "6e3159ac-9401-482d-d06a-b0097917d375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO : verification taille des donnees\n",
        "TODO: verification nombre d'epochs\n",
        "TODO : verification des logits\n",
        "TODO: synchronization des donnees par rapport a vikidia\n",
        "TODO : niveau 4\n",
        "TODO : linear regression for scores?"
      ],
      "metadata": {
        "id": "IDGTG-LJnMkh"
      }
    }
  ]
}